"""
Cell Culture Feature Extraction Task
Multi-model growth curve fitting with feature extraction
"""

import numpy as np
import pandas as pd
from typing import Dict, Callable, List
from scipy.optimize import least_squares
from numpy.linalg import inv, LinAlgError
import plotly.graph_objects as go

from gws_core import (ConfigParams, InputSpec, InputSpecs, OutputSpec,
                      OutputSpecs, Task, TaskInputs, TaskOutputs,
                      task_decorator, ConfigSpecs, Table, ResourceSet,
                      PlotlyResource, ListParam, Tag)


@task_decorator("CellCultureFeatureExtraction", human_name="Cell Culture Feature Extraction",
                short_description="Multi-model growth curve fitting with feature extraction (Logistic, Gompertz, Richards, Weibull, Baranyi-Roberts)")
class CellCultureFeatureExtraction(Task):
    """
    [Generated by Task Expert Agent]

    # Cell Culture Feature Extraction

    Performs **multi-model sigmoid fitting** on growth curve data to extract biological features.

    ## Description

    This task fits multiple growth models to time-series data (e.g., biomass vs temperature/time)
    and extracts key biological parameters and growth intervals.

    ### Supported Models (6)

    1. **Logistic (4 parameters)**: Classic sigmoid growth
    2. **Gompertz (4 parameters)**: Asymmetric growth with lag
    3. **Modified Gompertz (4 parameters)**: Alternative Gompertz formulation
    4. **Richards (5 parameters)**: Generalized logistic with shape parameter
    5. **Weibull Sigmoid (4 parameters)**: Weibull-based growth curve
    6. **Baranyi-Roberts (4 parameters)**: Microbial growth model

    ### Extracted Features

    **Model Parameters** (with 95% CI):
    - `y0`: Initial value
    - `A`: Asymptotic maximum value
    - `mu`: Growth rate parameter
    - `lag`: Lag phase duration
    - `nu`: Shape parameter (Richards only)

    **Statistical Metrics**:
    - R², Adjusted R², MSE, RMSE, MAE, AIC, BIC, SSE

    **Growth Intervals** (time points at % of amplitude):
    - t5, t10, t20, t50, t80, t90, t95
    - Delta_t_10_90, Delta_t_20_80, Delta_t_5_95 (growth phase durations)

    **Dynamic Features**:
    - `slope_max`: Maximum growth rate (dy/dt)
    - `t_at_slope_max`: Time at maximum slope
    - `mu_eff_max`: Effective specific growth rate
    - `doubling_time_mid`: Doubling time at mid-growth

    ## Inputs

    - **data_table**: Table with index column (time/temp) and one column per batch/sample

    ## Configuration

    - **models_to_fit**: List of models to test (default: all 6 models)

    ## Outputs

    - **results_table**: Table with all parameters, metrics, and growth intervals
    - **plots**: ResourceSet containing Plotly graphs (individual + comparative plots)

    ## Algorithm

    1. **Multi-start optimization**: 10 initial guesses per model for robustness
    2. **Robust fitting**: soft_l1 loss function to handle outliers
    3. **Confidence intervals**: 95% CI using Jacobian approximation
    4. **Growth analysis**: Numerical differentiation for slope and intervals

    ## Notes

    - All parameters are constrained to be strictly positive
    - Missing values (NaN) are automatically filtered
    - Outer join merges different time points across samples
    - Best fit selected based on lowest SSE across multi-start runs
    """

    # Constants
    EXP_CLIP = 80.0
    EPS_POS = 1e-9
    N_STARTS = 10
    N_PRED = 800
    ALPHA_CI = 0.95
    RNG_SEED = 42

    LSQ_KW = dict(
        loss="soft_l1",
        f_scale=1.0,
        max_nfev=200_000,
        xtol=1e-14,
        ftol=1e-14,
        gtol=1e-14,
        verbose=0
    )

    ALL_MODELS = [
        "Logistic_4P",
        "Gompertz_4P",
        "ModifiedGompertz_4P",
        "Richards_5P",
        "WeibullSigmoid_4P",
        "BaranyiRoberts_4P"
    ]

    input_specs = InputSpecs({
        'data_table': InputSpec(
            Table,
            human_name="Data Table",
            short_description="Table with index column (time/temp) and one column per batch/sample pair"
        )
    })

    output_specs = OutputSpecs({
        'results_table': OutputSpec(
            Table,
            human_name="Results Table",
            short_description="Table with model parameters, metrics, and growth intervals"
        ),
        'plots': OutputSpec(
            ResourceSet,
            human_name="Plots",
            short_description="ResourceSet containing all Plotly visualization graphs"
        )
    })

    config_specs = ConfigSpecs({
        'models_to_fit': ListParam(
            human_name="Models to Fit",
            short_description="List of growth models to test",
            default_value=ALL_MODELS
        )
    })

    def run(self, params: ConfigParams, inputs: TaskInputs) -> TaskOutputs:
        """Execute multi-model growth curve fitting"""

        data_table: Table = inputs['data_table']
        models_to_fit: List[str] = params.get_value('models_to_fit')

        self.log_info_message(f"Starting feature extraction with {len(models_to_fit)} models")
        self.update_progress_value(5, "Loading data")

        df = data_table.get_data()

        if df.shape[1] < 2:
            raise ValueError("Data table must have at least 2 columns (index + 1 data column)")

        x_col = df.columns[0]
        y_cols = list(df.columns[1:])

        self.log_info_message(f"Index column: '{x_col}', Data columns: {len(y_cols)}")

        rng = np.random.default_rng(self.RNG_SEED)

        summary_rows: List[Dict] = []
        plot_resource_set = ResourceSet()

        total_fits = len(y_cols) * len(models_to_fit)
        completed_fits = 0

        for y_col in y_cols:
            self.log_info_message(f"Processing series: {y_col}")

            x = pd.to_numeric(df[x_col], errors='coerce').to_numpy()
            y = pd.to_numeric(df[y_col], errors='coerce').to_numpy()

            mask = np.isfinite(x) & np.isfinite(y)
            tx, ty = x[mask], y[mask]

            if tx.size < 5:
                self.log_warning_message(f"Skipping '{y_col}': insufficient data points ({tx.size} < 5)")
                continue

            order = np.argsort(tx)
            tx, ty = tx[order], ty[order]

            series_results = {}
            for model_name in models_to_fit:
                self.log_info_message(f"  Fitting {model_name} to {y_col}")

                result = self._fit_one_model(tx, ty, model_name, rng)
                series_results[model_name] = result

                plot = self._create_model_plot(tx, ty, result, model_name, y_col, x_col)

                # Set name and add tags to individual model plot
                plot.name = f"{y_col} - {model_name} fit"
                plot.tags.add_tag(Tag("analysis_type", "growth_curve_fitting"))
                plot.tags.add_tag(Tag("analysis_task", "CellCultureFeatureExtraction"))
                plot.tags.add_tag(Tag("plot_type", "individual_fit"))
                plot.tags.add_tag(Tag("model_name", model_name))
                plot.tags.add_tag(Tag("series_name", y_col))
                plot.tags.add_tag(Tag("output_category", "visualization"))

                plot_resource_set.add_resource(plot, f"{y_col}__{model_name}")

                summary_rows.append(self._flatten_result(y_col, model_name, result))

                completed_fits += 1
                progress = 5 + int((completed_fits / total_fits) * 85)
                self.update_progress_value(progress, f"Fitted {completed_fits}/{total_fits}")

            if len(series_results) > 1:
                comp_plot = self._create_comparison_plot(tx, ty, series_results, y_col, x_col)

                # Set name and add tags to comparison plot
                comp_plot.name = f"{y_col} - Model comparison"
                comp_plot.tags.add_tag(Tag("analysis_type", "growth_curve_fitting"))
                comp_plot.tags.add_tag(Tag("analysis_task", "CellCultureFeatureExtraction"))
                comp_plot.tags.add_tag(Tag("plot_type", "comparison"))
                comp_plot.tags.add_tag(Tag("series_name", y_col))
                comp_plot.tags.add_tag(Tag("output_category", "visualization"))
                comp_plot.tags.add_tag(Tag("models_compared", str(len(series_results))))

                plot_resource_set.add_resource(comp_plot, f"{y_col}__comparison")

        self.update_progress_value(95, "Creating results table")

        if not summary_rows:
            raise ValueError("No valid results were generated. Check input data.")

        results_df = pd.DataFrame(summary_rows)
        results_table = Table(data=results_df)

        # Set name and add tags to results table
        results_table.name = "Growth curve fitting results"
        results_table.tags.add_tag(Tag("analysis_type", "growth_curve_fitting"))
        results_table.tags.add_tag(Tag("analysis_task", "CellCultureFeatureExtraction"))
        results_table.tags.add_tag(Tag("output_type", "parameters_metrics"))
        results_table.tags.add_tag(Tag("output_category", "results"))
        results_table.tags.add_tag(Tag("models_fitted", str(len(models_to_fit))))
        results_table.tags.add_tag(Tag("series_count", str(len(y_cols))))
        results_table.tags.add_tag(Tag("total_fits", str(len(summary_rows))))

        # Set name and add tags to plots ResourceSet
        plot_resource_set.name = "Growth curve fitting plots"
        plot_resource_set.tags.add_tag(Tag("analysis_type", "growth_curve_fitting"))
        plot_resource_set.tags.add_tag(Tag("analysis_task", "CellCultureFeatureExtraction"))
        plot_resource_set.tags.add_tag(Tag("output_category", "visualizations"))
        plot_resource_set.tags.add_tag(Tag("resource_count", str(len(plot_resource_set.get_resources()))))

        self.log_success_message(
            f"Completed {len(summary_rows)} model fits for {len(y_cols)} series"
        )

        self.update_progress_value(100, "Complete")

        return {
            'results_table': results_table,
            'plots': plot_resource_set
        }

    # ==================== MODEL DEFINITIONS ====================

    @staticmethod
    def _safe_exp(z: np.ndarray) -> np.ndarray:
        return np.exp(np.clip(z, -CellCultureFeatureExtraction.EXP_CLIP,
                              CellCultureFeatureExtraction.EXP_CLIP))

    @staticmethod
    def _logistic_4p(t, y0, A, mu, lag):
        exp = CellCultureFeatureExtraction._safe_exp
        return y0 + (A - y0) / (1.0 + exp(-mu * (t - lag)))

    @staticmethod
    def _gompertz_4p(t, y0, A, mu, lag):
        exp = CellCultureFeatureExtraction._safe_exp
        amp = np.maximum(A - y0, 1e-12)
        return y0 + amp * np.exp(-exp((mu * np.e / amp) * (lag - t) + 1.0))

    @staticmethod
    def _modified_gompertz_4p(t, y0, A, mu, lag):
        return CellCultureFeatureExtraction._gompertz_4p(t, y0, A, mu, lag)

    @staticmethod
    def _richards_5p(t, y0, A, mu, lag, nu):
        exp = CellCultureFeatureExtraction._safe_exp
        nu = np.maximum(nu, CellCultureFeatureExtraction.EPS_POS)
        return y0 + (A - y0) / (1.0 + nu * exp(-mu * (t - lag))) ** (1.0 / nu)

    @staticmethod
    def _weibull_sigmoid_4p(t, y0, A, mu, lag):
        amp = np.maximum(A - y0, 1e-12)
        tt = np.maximum(t - lag, 0.0)
        k = 2.0
        return y0 + amp * (1.0 - np.exp(-np.clip((mu * tt) ** k, 0.0, 1e6)))

    @staticmethod
    def _baranyi_roberts_4p(t, y0, A, mu, lag):
        exp = CellCultureFeatureExtraction._safe_exp
        amp = np.maximum(A - y0, 1e-12)
        z = mu * (t - lag)
        return y0 + amp / (1 + exp(-z)) * np.exp(-exp(-z))

    @classmethod
    def _get_model_dict(cls) -> Dict[str, Dict]:
        return {
            "Logistic_4P": {
                "fn": lambda t, p: cls._logistic_4p(t, *p),
                "p_names": ["y0", "A", "mu", "lag"],
            },
            "Gompertz_4P": {
                "fn": lambda t, p: cls._gompertz_4p(t, *p),
                "p_names": ["y0", "A", "mu", "lag"],
            },
            "ModifiedGompertz_4P": {
                "fn": lambda t, p: cls._modified_gompertz_4p(t, *p),
                "p_names": ["y0", "A", "mu", "lag"],
            },
            "Richards_5P": {
                "fn": lambda t, p: cls._richards_5p(t, *p),
                "p_names": ["y0", "A", "mu", "lag", "nu"],
            },
            "WeibullSigmoid_4P": {
                "fn": lambda t, p: cls._weibull_sigmoid_4p(t, *p),
                "p_names": ["y0", "A", "mu", "lag"],
            },
            "BaranyiRoberts_4P": {
                "fn": lambda t, p: cls._baranyi_roberts_4p(t, *p),
                "p_names": ["y0", "A", "mu", "lag"],
            },
        }

    # ==================== FITTING LOGIC ====================

    def _fit_one_model(self, x: np.ndarray, y: np.ndarray, model_name: str,
                       rng: np.random.Generator) -> Dict:

        models_dict = self._get_model_dict()
        model_info = models_dict[model_name]
        model_fn = model_info["fn"]
        p_names = model_info["p_names"]

        is_richards = (model_name == "Richards_5P")
        bounds = self._build_bounds(x, y, is_richards)
        base = self._heuristic_initials(x, y, is_richards)

        starts = self._multistart_guesses(base, bounds[0], bounds[1], self.N_STARTS, rng)

        best_res = None
        best_sse = np.inf

        for guess in starts:
            try:
                res = least_squares(
                    fun=lambda p: model_fn(x, p) - y,
                    x0=guess,
                    bounds=bounds,
                    **self.LSQ_KW
                )
                sse = float(np.sum(res.fun ** 2))
                if res.success and sse < best_sse:
                    best_res = res
                    best_sse = sse
            except Exception:
                continue

        if best_res is None:
            return self._empty_result(model_name, p_names)

        p_opt = best_res.x
        y_fit = model_fn(x, p_opt)
        metrics = self._compute_metrics(y, y_fit, len(p_names))

        dof = max(len(y) - len(p_names), 1)
        se, cov, q = self._param_ci(best_res.jac, best_res.fun, dof)

        if se is not None and np.isfinite(q):
            p_lo = np.maximum(p_opt - q * se, self.EPS_POS)
            p_hi = np.maximum(p_opt + q * se, self.EPS_POS)
        else:
            p_lo = np.full_like(p_opt, np.nan)
            p_hi = np.full_like(p_opt, np.nan)

        t_pred = np.linspace(float(np.min(x)), float(np.max(x)), self.N_PRED)
        yhat, plo, phi = self._prediction_ci(model_fn, t_pred, p_opt, cov, q)

        growth_iv = self._growth_intervals(model_fn, p_opt, float(np.min(x)), float(np.max(x)))

        return {
            "success": True,
            "model": model_name,
            "params": p_opt,
            "params_se": se if se is not None else np.full(len(p_names), np.nan),
            "params_lo": p_lo,
            "params_hi": p_hi,
            "metrics": metrics,
            "pred_t": t_pred,
            "pred": yhat,
            "pred_lo": plo,
            "pred_hi": phi,
            "growth_intervals": growth_iv
        }

    def _empty_result(self, model_name: str, p_names: List[str]) -> Dict:
        n = len(p_names)
        return {
            "success": False,
            "model": model_name,
            "params": np.full(n, np.nan),
            "params_se": np.full(n, np.nan),
            "params_lo": np.full(n, np.nan),
            "params_hi": np.full(n, np.nan),
            "metrics": {k: np.nan for k in ["R2", "Adj_R2", "MSE", "RMSE", "MAE", "AIC", "BIC", "SSE", "N"]},
            "pred_t": None,
            "pred": None,
            "pred_lo": None,
            "pred_hi": None,
            "growth_intervals": {k: np.nan for k in [
                "t5", "t10", "t20", "t50", "t80", "t90", "t95",
                "Delta_t_10_90", "Delta_t_20_80", "Delta_t_5_95",
                "slope_max", "t_at_slope_max", "mu_eff_max", "doubling_time_mid"
            ]}
        }

    # ==================== PARAMETER ESTIMATION ====================

    def _heuristic_initials(self, x: np.ndarray, y: np.ndarray, is_richards: bool) -> np.ndarray:
        y0 = float(np.nanpercentile(y, 5))
        A = float(np.nanpercentile(y, 95))
        A = max(A, y0 + 0.1 * (np.nanmax(y) - np.nanmin(y)))

        if len(x) >= 3:
            dy = np.gradient(y, x, edge_order=2)
            max_slope = float(np.nanmax(dy)) if np.isfinite(dy).any() else 1.0
        else:
            max_slope = (np.nanmax(y) - np.nanmin(y)) / max(np.nanmax(x) - np.nanmin(x), 1.0)

        mu = max(max_slope / max(A - y0, 1e-6), 1e-4)

        amp = A - y0
        target = y0 + 0.1 * amp
        idx = int(np.argmin(np.abs(y - target)))
        lag = float(x[idx]) if np.isfinite(x[idx]) else float(np.nanmedian(x))

        if is_richards:
            base = np.array([y0, A, mu, lag, 1.0], dtype=float)
        else:
            base = np.array([y0, A, mu, lag], dtype=float)

        return np.maximum(base, self.EPS_POS)

    def _build_bounds(self, x: np.ndarray, y: np.ndarray, is_richards: bool) -> tuple:
        tmin, tmax = float(np.nanmin(x)), float(np.nanmax(x))
        ymin, ymax = float(np.nanmin(y)), float(np.nanmax(y))
        yr = max(ymax - ymin, 1e-6)
        span_t = max(tmax - tmin, 1e-3)

        y0_low = self.EPS_POS
        A_low = self.EPS_POS
        mu_low = self.EPS_POS
        lag_low = self.EPS_POS
        nu_low = self.EPS_POS

        y0_high = max(ymin + 0.75 * yr, self.EPS_POS) + 10.0 * max(1.0, abs(ymin))
        A_high = max(ymax + 2.0 * yr, self.EPS_POS) + 10.0 * max(1.0, abs(ymax))
        mu_high = max(20.0, 20.0 / span_t)
        lag_high = max(tmax + 2.0 * span_t + 10.0, lag_low + 1.0)
        nu_high = 20.0

        if is_richards:
            lower = np.array([y0_low, A_low, mu_low, lag_low, nu_low])
            upper = np.array([y0_high, A_high, mu_high, lag_high, nu_high])
        else:
            lower = np.array([y0_low, A_low, mu_low, lag_low])
            upper = np.array([y0_high, A_high, mu_high, lag_high])

        return (lower, upper)

    def _multistart_guesses(self, base: np.ndarray, lower: np.ndarray, upper: np.ndarray,
                            n: int, rng: np.random.Generator) -> np.ndarray:
        guesses = []
        for _ in range(n):
            r = rng.normal(0.0, 0.35, size=base.shape)
            g = base.copy()
            for j in range(len(base)):
                lo, hi = lower[j], upper[j]
                g[j] = np.clip(g[j], lo, hi)
                if j in (2, 4):  # mu or nu
                    val = max(g[j], self.EPS_POS)
                    val *= np.exp(r[j])
                    g[j] = np.clip(val, lo + self.EPS_POS, hi - self.EPS_POS)
                else:
                    span = hi - lo
                    g[j] = np.clip(g[j] + r[j] * 0.25 * span, lo, hi)
            guesses.append(g)
        return np.vstack(guesses)

    # ==================== STATISTICS ====================

    def _compute_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, p_count: int) -> Dict:
        n = len(y_true)
        resid = y_true - y_pred
        ss_res = float(np.sum(resid ** 2))
        ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2)) if n > 1 else np.nan

        r2 = 1 - ss_res / ss_tot if (ss_tot > 0 and np.isfinite(ss_tot)) else np.nan
        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p_count - 1) if n > p_count + 1 and np.isfinite(r2) else np.nan

        mse = ss_res / n if n > 0 else np.nan
        rmse = float(np.sqrt(mse)) if np.isfinite(mse) else np.nan
        mae = float(np.mean(np.abs(resid))) if n > 0 else np.nan

        sigma2 = mse
        if n > 0 and np.isfinite(sigma2) and sigma2 > 0:
            aic = n * (np.log(2 * np.pi) + 1) + n * np.log(sigma2) + 2 * p_count
            bic = n * (np.log(2 * np.pi) + 1) + n * np.log(sigma2) + p_count * np.log(n)
        else:
            aic = np.nan
            bic = np.nan

        return dict(R2=r2, Adj_R2=adj_r2, MSE=mse, RMSE=rmse, MAE=mae, AIC=aic, BIC=bic, SSE=ss_res, N=n)

    def _param_ci(self, J, resid_vec, dof, alpha=0.95):
        if J is None or dof <= 0:
            return (None, None, np.nan)
        try:
            JTJ = J.T @ J
            cov = inv(JTJ)
            s2 = float(np.sum(resid_vec ** 2)) / dof
            cov *= s2
            se = np.sqrt(np.diag(cov))
            from scipy.stats import t as student_t
            q = float(student_t.ppf(0.5 + alpha / 2.0, dof)) if dof > 0 else 1.96
            return se, cov, q
        except LinAlgError:
            return (None, None, np.nan)

    def _numerical_grad(self, model_fn: Callable, t: np.ndarray, p: np.ndarray, eps: float = 1e-6) -> np.ndarray:
        f0 = model_fn(t, p)
        J = np.zeros((t.size, p.size), dtype=float)
        for i in range(p.size):
            dp = np.zeros_like(p)
            step = eps * max(abs(p[i]), 1.0)
            dp[i] = step
            f1 = model_fn(t, p + dp)
            J[:, i] = (f1 - f0) / step
        return J

    def _prediction_ci(self, model_fn: Callable, t: np.ndarray, p: np.ndarray, cov, q: float):
        yhat = model_fn(t, p)
        if cov is None or not isinstance(cov, np.ndarray) or not np.all(np.isfinite(cov)):
            return yhat, np.full_like(yhat, np.nan), np.full_like(yhat, np.nan)

        Jpred = self._numerical_grad(model_fn, t, p)
        var = np.einsum("ij,jk,ik->i", Jpred, cov, Jpred)
        se = np.sqrt(np.maximum(var, 0.0))
        lo = yhat - q * se
        hi = yhat + q * se
        return yhat, lo, hi

    # ==================== GROWTH INTERVALS ====================

    def _find_time_for_level(self, t_grid: np.ndarray, y_grid: np.ndarray, level: float) -> float:
        y = y_grid - level
        s = np.sign(y)
        idx = np.where(np.diff(s) != 0)[0]
        if idx.size == 0:
            return np.nan
        i = int(idx[0])
        x0, x1 = t_grid[i], t_grid[i + 1]
        y0, y1 = y_grid[i], y_grid[i + 1]
        if y1 == y0:
            return float(x0)
        frac = (level - y0) / (y1 - y0)
        return float(x0 + frac * (x1 - x0))

    def _growth_intervals(self, model_fn: Callable, p: np.ndarray, t_min: float, t_max: float) -> Dict[str, float]:
        t_grid = np.linspace(t_min, t_max, 2000)
        y_grid = model_fn(t_grid, p)

        y0 = float(p[0])
        A = float(p[1])
        amp = max(A - y0, 1e-12)

        levels = {
            "t5": y0 + 0.05 * amp,
            "t10": y0 + 0.10 * amp,
            "t20": y0 + 0.20 * amp,
            "t50": y0 + 0.50 * amp,
            "t80": y0 + 0.80 * amp,
            "t90": y0 + 0.90 * amp,
            "t95": y0 + 0.95 * amp,
        }
        times = {k: self._find_time_for_level(t_grid, y_grid, v) for k, v in levels.items()}

        dy_dt = np.gradient(y_grid, t_grid, edge_order=2)
        jmax = int(np.nanargmax(dy_dt)) if np.isfinite(dy_dt).any() else 0
        slope_max = float(dy_dt[jmax])
        t_at_slope_max = float(t_grid[jmax])

        mu_eff_max = slope_max / amp if np.isfinite(slope_max) else np.nan
        doubling_time_mid = (np.log(2.0) / mu_eff_max) if (mu_eff_max > 0 and np.isfinite(mu_eff_max)) else np.nan

        def delta(a: float, b: float) -> float:
            return (b - a) if (np.isfinite(a) and np.isfinite(b)) else np.nan

        return {
            **times,
            "Delta_t_10_90": delta(times["t10"], times["t90"]),
            "Delta_t_20_80": delta(times["t20"], times["t80"]),
            "Delta_t_5_95": delta(times["t5"], times["t95"]),
            "slope_max": slope_max,
            "t_at_slope_max": t_at_slope_max,
            "mu_eff_max": mu_eff_max,
            "doubling_time_mid": doubling_time_mid,
        }

    # ==================== OUTPUT FORMATTING ====================

    def _flatten_result(self, series_name: str, model_name: str, result: Dict) -> Dict:
        models_dict = self._get_model_dict()
        p_names = models_dict[model_name]["p_names"]

        params = result["params"]
        se = result["params_se"]
        plo = result["params_lo"]
        phi = result["params_hi"]
        gi = result.get("growth_intervals", {}) or {}

        row = {
            "Series": series_name, "Model": model_name, **
            {f"param_{name}": (params[i] if i < len(params) else np.nan) for i, name in enumerate(p_names)},
            **
            {f"SE_{name}": (se[i] if isinstance(se, (list, np.ndarray)) and i < len(se) else np.nan) for i,
             name in enumerate(p_names)},
            **{f"CIlo_{name}": (plo[i] if i < len(plo) else np.nan) for i, name in enumerate(p_names)},
            **{f"CIhi_{name}": (phi[i] if i < len(phi) else np.nan) for i, name in enumerate(p_names)},
            **result["metrics"],
            **
            {k: gi.get(k, np.nan)
             for k
             in
             ["t5", "t10", "t20", "t50", "t80", "t90", "t95", "Delta_t_10_90", "Delta_t_20_80", "Delta_t_5_95",
              "slope_max", "t_at_slope_max", "mu_eff_max", "doubling_time_mid"]},
            "Success": result["success"], }
        return row

    # ==================== PLOTTING ====================

    def _create_model_plot(self, x: np.ndarray, y: np.ndarray, result: Dict,
                           model_name: str, series_name: str, x_label: str) -> PlotlyResource:
        fig = go.Figure()

        fig.add_trace(go.Scatter(
            x=x, y=y, mode="markers",
            name=f"{series_name} data",
            marker=dict(size=6)
        ))

        if result["success"] and result["pred_t"] is not None:
            fig.add_trace(go.Scatter(
                x=result["pred_t"], y=result["pred"],
                mode="lines", name=f"{model_name} fit",
                line=dict(width=2)
            ))

            if result["pred_lo"] is not None and np.all(np.isfinite(result["pred_lo"])):
                fig.add_trace(go.Scatter(
                    x=result["pred_t"], y=result["pred_hi"],
                    mode="lines", line=dict(width=0),
                    showlegend=False, hoverinfo="skip"
                ))
                fig.add_trace(go.Scatter(
                    x=result["pred_t"], y=result["pred_lo"],
                    mode="lines", line=dict(width=0),
                    fill="tonexty", name="95% CI",
                    fillcolor="rgba(0,100,200,0.2)",
                    hoverinfo="skip"
                ))

        if result["success"]:
            metrics = result["metrics"]
            annotation_text = (
                f"R² = {metrics['R2']:.4f}<br>"
                f"RMSE = {metrics['RMSE']:.4f}<br>"
                f"AIC = {metrics['AIC']:.1f}"
            )
            fig.add_annotation(
                xref="paper", yref="paper",
                x=0.02, y=0.98,
                text=annotation_text,
                showarrow=False,
                bgcolor="white",
                bordercolor="black",
                borderwidth=1,
                xanchor="left", yanchor="top"
            )

        fig.update_layout(
            title=f"{series_name} – {model_name}",
            xaxis_title=x_label,
            yaxis_title=series_name,
            template="plotly_white",
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
        )

        return PlotlyResource(fig)

    def _create_comparison_plot(self, x: np.ndarray, y: np.ndarray, results: Dict[str, Dict],
                                series_name: str, x_label: str) -> PlotlyResource:
        fig = go.Figure()

        fig.add_trace(go.Scatter(
            x=x, y=y, mode="markers",
            name=f"{series_name} data",
            marker=dict(size=6, color="black")
        ))

        for model_name, result in results.items():
            if result["success"] and result["pred_t"] is not None:
                fig.add_trace(go.Scatter(
                    x=result["pred_t"], y=result["pred"],
                    mode="lines", name=model_name,
                    line=dict(width=2)
                ))

        fig.update_layout(
            title=f"{series_name} – Model Comparison",
            xaxis_title=x_label,
            yaxis_title=series_name,
            template="plotly_white",
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
        )

        return PlotlyResource(fig)
