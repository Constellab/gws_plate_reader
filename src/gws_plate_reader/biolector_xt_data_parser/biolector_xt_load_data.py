import json
import os
from typing import Any

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from gws_core import (
    ConfigParams,
    ConfigSpecs,
    DynamicInputs,
    Folder,
    InputSpec,
    InputSpecs,
    ListParam,
    OutputSpec,
    OutputSpecs,
    PlotlyResource,
    ResourceSet,
    StrParam,
    Table,
    Task,
    TaskInputs,
    TaskOutputs,
    TypingStyle,
    task_decorator,
)
from gws_core.resource.resource_set.resource_list import ResourceList
from gws_core.tag.tag import Tag, TagOrigins
from gws_core.tag.tag_dto import TagOriginType
from gws_core.user.current_user_service import CurrentUserService
from pandas import NA, DataFrame, Series

DOWNLOAD_TAG_KEY = "biolector_download"


def create_venn_diagram_wells(well_sets: dict[str, set[str]]) -> go.Figure:
    """
    Create a Venn diagram showing data availability across wells.

    Args:
        well_sets: Dict with sets of well identifiers for each data type
            Keys: 'cultivation_labels', 'medium_info', 'raw_data'
            Values: Set of well identifiers (e.g., 'A01', 'B02')

    Returns:
        Plotly Figure with the Venn diagram showing well data completeness
    """

    # Extraire les ensembles
    A = well_sets.get('cultivation_labels', set())  # CultivationLabels
    B = well_sets.get('raw_data', set())  # raw_data

    # Calculer les r√©gions pour 2-cercles
    only_A = len(A - B)  # Seulement CultivationLabels
    only_B = len(B - A)  # Seulement raw_data
    both = len(A & B)    # Intersection

    # Cr√©er la figure
    fig = go.Figure()

    # Param√®tres des cercles (horizontal)
    radius = 0.28
    Ax, Ay = 0.35, 0.5   # CultivationLabels (gauche)
    Bx, By = 0.65, 0.5   # raw_data (droite)

    theta = np.linspace(0, 2 * np.pi, 100)

    # Cercle A - CultivationLabels (Bleu)
    x_A = radius * np.cos(theta) + Ax
    y_A = radius * np.sin(theta) + Ay
    fig.add_trace(go.Scatter(
        x=x_A, y=y_A,
        fill='toself',
        fillcolor='rgba(33, 150, 243, 0.3)',
        line=dict(color='rgba(33, 150, 243, 0.8)', width=3),
        name='CultivationLabels',
        mode='lines',
        hoverinfo='skip',
        showlegend=False
    ))

    # Cercle B - raw_data (Orange)
    x_B = radius * np.cos(theta) + Bx
    y_B = radius * np.sin(theta) + By
    fig.add_trace(go.Scatter(
        x=x_B, y=y_B,
        fill='toself',
        fillcolor='rgba(255, 152, 0, 0.3)',
        line=dict(color='rgba(255, 152, 0, 0.8)', width=3),
        name='raw_data',
        mode='lines',
        hoverinfo='skip',
        showlegend=False
    ))

    # Titres au-dessus de chaque cercle
    fig.add_annotation(x=Ax, y=Ay + radius + 0.05, text="<b>CultivationLabels</b>",
                       showarrow=False, font=dict(size=14))
    fig.add_annotation(x=Bx, y=By + radius + 0.05, text="<b>raw_data</b>",
                       showarrow=False, font=dict(size=14))

    # Compteurs de r√©gions
    fig.add_annotation(x=Ax - 0.13, y=Ay, text=str(only_A),
                       showarrow=False, font=dict(size=14))
    fig.add_annotation(x=Bx + 0.13, y=By, text=str(only_B),
                       showarrow=False, font=dict(size=14))
    fig.add_annotation(x=(Ax + Bx) / 2, y=Ay, text=f"<b>{both}</b>",
                       showarrow=False, font=dict(size=16, color='darkgreen'),
                       bgcolor='rgba(255, 255, 255, 0.9)',
                       borderpad=4,
                       bordercolor='darkgreen',
                       borderwidth=2)

    # Mettre √† jour le layout
    fig.update_layout(
        title="Well Data Availability - Venn Diagram (CultivationLabels, raw_data)",
        showlegend=False,
        xaxis=dict(showticklabels=False, showgrid=False, zeroline=False, range=[0, 1]),
        yaxis=dict(
            showticklabels=False, showgrid=False, zeroline=False, range=[0.2, 0.8],
            scaleanchor="x", scaleratio=1),
        height=500, width=600,
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
    )

    return fig


@task_decorator("BiolectorXTLoadData", human_name="BiolectorXT Load Data",
                short_description="Load and process BiolectorXT data with quality control visualization",
                style=TypingStyle.community_icon(icon_technical_name="file-upload", background_color="#c3fa7f"))
class BiolectorXTLoadData(Task):
    """
    [Generated by Task Expert Agent]

    Load and process BiolectorXT data from raw measurements and metadata files.

    ## Overview
    This task integrates BiolectorXT data from raw measurements and metadata to create a comprehensive dataset
    for microplate analysis. It handles data parsing, well labeling, quality control visualization,
    and generates statistics about data completeness.

    ## Input Files Required

    ### 1. Raw Data Table (`raw_data`)
    Table containing raw measurement data from BiolectorXT with columns:
    - `Well`: Well identifier (e.g., "A01", "B02")
    - `Filterset`: Filter/channel name (e.g., "Biomass", "pH", "DO")
    - `Time`: Measurement time in seconds
    - `Cal`: Calibrated measurement value
    - Additional metadata columns may be present

    ### 2. Metadata Folder (`folder_metadata`)
    Folder containing JSON metadata file(s) ending with 'BXT.json':
    - **Channels**: List of measurement channels/filters
    - **Microplate**: Well configuration
      - `CultivationLabels`: Wells used for cultivation
      - `ReservoirLabels`: Wells used as reservoirs
    - **Layout**: Well label descriptions
      - `CultivationLabelDescriptionsMap`: Descriptions for cultivation wells
      - `ReservoirLabelDescriptionsMap`: Descriptions for reservoir wells
    - **Comment**: Experiment comment/description
    - **Name**: Experiment name
    - **UserName**: User who created the experiment
    - **LastModifiedAt**: Last modification date

    ### 3. Plate Layout (Optional) (`plate_layout`)
    JSONDict containing custom well labels and additional metadata:
    - Keys: Well identifiers (e.g., "A1", "A01")
    - Values: Dict with `label` key and optional additional metadata
    - Overrides metadata labels if provided

    ## Processing Steps

    1. **Metadata Extraction**: Reads BXT.json file from metadata folder
    2. **Data Parsing**: Transforms raw data from long to wide format
       - Groups by Filterset to separate different measurement channels
       - Creates intermediate tables with wells as columns
       - Handles both microfluidics (C01-F08) and standard (A01-F08) layouts
    3. **Data Restructuring**: Pivots data to create one table per well
       - Each table contains time column and all measurement channels
       - Filters out wells with no data (all NaN)
    4. **Well Labeling**: Merges labels from metadata and optional plate layout
    5. **Quality Control**: Tracks well data availability
       - Cultivation wells
       - Reservoir wells
       - Labeled wells
    6. **Tagging**: Adds comprehensive tags from metadata
       - Batch (experiment/plate name) and sample (well ID)
       - Experiment name, comment, user, date
       - Raw data source
       - Well-specific labels and metadata
    6. **Statistics**: Generates metadata summary table with well counts

    ## Outputs

    ### 1. Parsed Data Tables (`parsed_data_tables`)
    A ResourceSet containing one Table per well (batch/sample combination):
    - **Table Name**: Well identifier (e.g., "A01", "B02", "C03")
    - **Columns**:
      - `Temps_en_h`: Time in hours
      - One column per measurement channel (e.g., "Biomass", "pH", "pO2", "DO")
    - **Resource Tags**:
      - `batch`: Experiment/plate name (from metadata Name field)
      - `sample`: Well identifier (e.g., "A01")
      - `label`: Well label/description (if available)
      - `comment`: Experiment comment
      - `name`: Experiment name
      - `user_name`: User who created the experiment
      - `date`: Last modification date
      - `raw_data`: Raw data table name
      - `biolector_download`: Download tag (if present in raw data)
      - Additional custom metadata from plate_layout

    ### 2. Venn Diagram (`venn_diagram`) - Optional
    A PlotlyResource containing an interactive Venn diagram showing:
    - **3 Overlapping Circles**: Cultivation, Reservoir, Labeled
    - **Circle Labels**: Show count of wells in each category
    - **Center Label**: Shows count of fully characterized wells (all 3 categories)
    - **Color Coding**:
      - Blue: Cultivation wells
      - Green: Reservoir wells
      - Purple: Labeled wells

    ### 3. Metadata Summary Table (`metadata_summary`) - Optional
    A Table containing experiment-level statistics:
    - **Columns**:
      - `metric`: Metric name
      - `value`: Metric value
    - **Metrics**:
      - Total channels/filters
      - Total wells
      - Cultivation wells count
      - Reservoir wells count
      - Labeled wells count
      - Experiment name
      - User name
      - Comment
      - Last modified date

    ## Data Quality

    ### Well Type Detection
    The task automatically identifies:
    - **Cultivation wells**: From metadata CultivationLabels
    - **Reservoir wells**: From metadata ReservoirLabels
    - **Labeled wells**: Wells with non-empty labels from metadata or plate layout

    ### Microfluidics Detection
    Automatically detects microfluidics mode:
    - Checks if "A01" is present in well identifiers
    - If not present: Microfluidics mode (C01-F08 wells)
    - If present: Standard mode (A01-F08 wells)

    ## Use Cases

    1. **Quality Control**: Use Venn diagram to assess well labeling completeness
    2. **Data Exploration**: Browse parsed data with proper well labels
    3. **Batch Processing**: Process multiple experiments with consistent structure
    4. **Dashboard Preparation**: Provides clean, tagged data ready for visualization
    5. **Downstream Analysis**: Standardized format for filtering, analysis tasks

    ## Example Workflow

    ```
    [BiolectorXT Download] ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ> [Raw Table]
                             ‚îî‚îÄ‚îÄ> [Metadata Folder]
                                      ‚îÇ
                                      ‚îÇ
    [Plate Layout JSON] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ> BiolectorXTLoadData ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ> [Well Tables] ‚îÄ‚îÄ> Filter/Analysis
                                                                  ‚îú‚îÄ‚îÄ> [Venn Diagram] ‚îÄ‚îÄ> QC Report
                                                                  ‚îî‚îÄ‚îÄ> [Metadata Summary] ‚îÄ‚îÄ> Stats
    ```

    ## Example Output Structure

    For a plate with 3 wells (A01, A02, B01) and 2 measurements (Biomass, pH):
    ```
    ResourceSet containing 3 tables:

    Table "A01":
      Temps_en_h | Biomass | pH
      0.0        | 0.123   | 7.2
      0.5        | 0.156   | 7.1
      1.0        | 0.198   | 7.0

    Table "A02":
      Temps_en_h | Biomass | pH
      0.0        | 0.115   | 7.3
      0.5        | 0.142   | 7.2
      ...
    ```

    ## Notes

    - All JSON files must be UTF-8 encoded
    - Metadata file must end with 'BXT.json'
    - Well identifiers are normalized (e.g., "A1" ‚Üí "A01")
    - Wells with no data (all NaN) are excluded from the output
    - Time is provided in hours (Temps_en_h)
    - Each table represents one well with all its measurements
    - Output format is compatible with filtering and analysis tasks
    - Plate layout overrides metadata labels when provided
    - Tags include batch (experiment name) and sample (well ID) for easy filtering

    ## Comparison with BiolectorXTDataParser

    This task differs from BiolectorXTDataParser:
    - **Different output structure**: One table per well instead of one table per channel
    - **Added features**:
      - Venn diagram for data availability visualization
      - Metadata summary table with experiment statistics
      - Batch/sample tagging for consistent data organization
    - **Enhanced documentation** with usage examples
    - **Consistent naming** with other load tasks (e.g., FermentalgLoadData)
    """

    input_specs: InputSpecs = DynamicInputs(
        default_specs={
            'medium_table': InputSpec(
                Table,
                human_name="Medium composition table",
                short_description="Table with medium compositions (Medium, Component1, Component2, ...)",
                optional=True
            )
        },
        additionnal_port_spec=InputSpec(
            ResourceSet,
            human_name="Plate data (raw_data, folder_metadata, info_table)",
            short_description="ResourceSet containing: raw_data (Table), folder_metadata (Folder), info_table (Table)"
        )
    )

    config_specs: ConfigSpecs = ConfigSpecs({
        'plate_names': ListParam(
            human_name="Plate names",
            short_description="Custom names for each plate. Leave empty to use default names (plate_0, plate_1, etc.). Must match the number of input plates if provided.",
            optional=True,
            default_value=[]
        )
    })

    output_specs: OutputSpecs = OutputSpecs({
        'resource_set': OutputSpec(
            ResourceSet,
            human_name="Parsed data tables resource set",
            short_description="One table per well with all measurement channels as columns",
            sub_class=True
        ),
        'venn_diagram': OutputSpec(
            PlotlyResource,
            human_name="Venn diagram of well data availability",
            short_description="Visual representation of cultivation, reservoir, and labeled wells",
            optional=True
        ),
        'metadata_table': OutputSpec(
            Table,
            human_name="Metadata table for ML",
            short_description="Table with well metadata for feature extraction",
            optional=True
        ),
        'medium_table': OutputSpec(
            Table,
            human_name="Medium composition table",
            short_description="Table with unique medium compositions (output when medium_table provided as input)",
            optional=True
        )
    })

    def is_micro_fluidics(self, data: DataFrame) -> bool:
        """
        Check if the data is from a microfluidics experiment.

        :param data: Raw data DataFrame
        :return: True if microfluidics, False otherwise
        """
        unique_wells = data['Well'].dropna().unique()
        if "A01" in unique_wells:
            return False
        return True

    def get_filters(self, metadata: dict) -> list[str]:
        """
        Get the list of measurement filters/channels from metadata.

        :param metadata: Metadata dictionary
        :return: List of filter names
        """
        filters = []
        for channel in metadata.get('Channels', []):
            filters.append(channel['Name'])
        return filters

    def parse_data(self, data: DataFrame, metadata: dict) -> dict[str, DataFrame]:
        """
        Parse the raw data from BiolectorXT into wide format tables.

        :param data: Raw data DataFrame
        :param metadata: Metadata dictionary
        :return: Dictionary mapping filter names to parsed DataFrames
        """
        self.log_info_message("üîç [PARSE_DATA] Starting parse_data...")
        self.log_info_message(f"üìä [PARSE_DATA] Input data shape: {data.shape}")
        self.log_info_message(f"üìã [PARSE_DATA] Input data columns: {list(data.columns)}")
        self.log_info_message(f"üî¢ [PARSE_DATA] First few rows of data:\n{data.head(3)}")

        is_micro_fluidics: bool = self.is_micro_fluidics(data)
        filters: list[str] = self.get_filters(metadata)

        self.log_info_message(f"üß™ [PARSE_DATA] Microfluidics mode: {is_micro_fluidics}")
        self.log_info_message(f"üé® [PARSE_DATA] Filters detected: {filters}")

        # Sort and filter data
        row_data = data.sort_values(by=['Filterset', 'Well'])
        reduced_data = row_data[["Well", "Filterset", "Time", "Cal"]]
        unique_values = reduced_data['Filterset'].dropna().unique()

        self.log_info_message(f"üì¶ [PARSE_DATA] Unique filterset values: {list(unique_values)}")
        self.log_info_message(f"üìä [PARSE_DATA] Reduced data shape: {reduced_data.shape}")

        df_filter_dict: dict[str, DataFrame] = {}

        for i, value in enumerate(unique_values):
            self.log_info_message(f"\nüîÑ [PARSE_DATA] Processing filter {i+1}/{len(unique_values)}: {value}")
            df_filter = reduced_data[reduced_data['Filterset'] == value]
            df_filter = df_filter.sort_values(by=['Well', 'Time'])
            df_filter = df_filter.drop(columns="Filterset")

            # Determine well range based on microfluidics mode
            if is_micro_fluidics:
                columns_to_add = [f"{chr(letter)}{str(num).zfill(2)}"
                                  for letter in range(ord('C'), ord('F') + 1)
                                  for num in range(1, 9)]
            else:
                columns_to_add = [f"{chr(letter)}{str(num).zfill(2)}"
                                  for letter in range(ord('A'), ord('F') + 1)
                                  for num in range(1, 9)]

            # Save original time column before we overwrite it
            original_time_col = df_filter['Time'].copy()

            # Add columns for time and wells
            # Create only 'Time' column in hours (standardized)
            df_filter = df_filter.assign(Time=NA, **{col: NA for col in columns_to_add})

            # Fill Time column in hours
            df_filter["Time"] = original_time_col / 3600

            # Populate well columns
            for name_col in columns_to_add:
                df_filter[name_col] = df_filter.loc[df_filter['Well'] == name_col, 'Cal']

            # Shift values up to remove NaN rows
            columns_to_process = df_filter.columns[3:len(df_filter.columns)]
            df_filter = df_filter.reset_index(drop=True)

            for col in columns_to_process:
                df_filter[col] = Series(df_filter[col].dropna().values)

            # Clean up - keep standardized 'Time' column (in hours)
            df_filter = df_filter.dropna(subset=['Time'])
            self.log_info_message(f"üìä [PARSE_DATA] After dropna, shape: {df_filter.shape}")

            df_filter = df_filter.drop(columns=["Well", "Cal"])
            self.log_info_message(f"üìä [PARSE_DATA] After drop columns, shape: {df_filter.shape}")
            self.log_info_message(f"üìã [PARSE_DATA] Final columns: {list(df_filter.columns)}")

            # Add to dictionary
            if i < len(filters):
                filter_name = filters[i]
                df_filter_dict[filter_name] = df_filter
                self.log_info_message(f"‚úÖ [PARSE_DATA] Added filter '{filter_name}' with shape {df_filter.shape}")
            else:
                self.log_warning_message(f"‚ö†Ô∏è [PARSE_DATA] Filter index {i} >= len(filters) {len(filters)}, skipping")

        self.log_info_message(f"\n‚úÖ [PARSE_DATA] parse_data completed. Generated {len(df_filter_dict)} filter tables")
        return df_filter_dict

    def get_wells_cultivation(self, metadata: dict) -> list[str]:
        """Get cultivation wells from metadata."""
        microplate = metadata.get("Microplate", {})
        return microplate.get("CultivationLabels", [])

    def get_wells_reservoir(self, metadata: dict) -> list[str]:
        """Get reservoir wells from metadata."""
        microplate = metadata.get("Microplate", {})
        return microplate.get("ReservoirLabels", [])

    def get_wells(self, metadata: dict) -> list[str]:
        """Get all wells (cultivation + reservoir) from metadata."""
        wells = []
        wells.extend(self.get_wells_cultivation(metadata))
        wells.extend(self.get_wells_reservoir(metadata))
        return wells

    def get_wells_label_description(
            self, metadata: dict, existing_plate_layout: dict | None = None) -> dict[str, Any]:
        """
        Get well labels and descriptions from metadata and optional plate layout.

        :param metadata: Metadata dictionary
        :param existing_plate_layout: Optional plate layout override
        :return: Dictionary mapping well IDs to their metadata
        """
        # Create all possible wells A01 to F08
        wells = [f"{chr(letter)}{str(num).zfill(2)}"
                 for letter in range(ord('A'), ord('F') + 1)
                 for num in range(1, 9)]
        wells_label = {well: {"label": ""} for well in wells}

        # Get labels from metadata
        microplate = metadata.get("Layout", {})
        cultivation_map = microplate.get("CultivationLabelDescriptionsMap", {})
        reservoir_map = microplate.get("ReservoirLabelDescriptionsMap", {})

        for well, description in cultivation_map.items():
            if well in wells_label:
                wells_label[well] = {"label": description.strip() or wells_label[well]}

        for well, description in reservoir_map.items():
            if well in wells_label:
                wells_label[well] = {"label": description.strip() or wells_label[well]}

        # Override with plate layout if provided
        if existing_plate_layout:
            for well, data in existing_plate_layout.items():
                # Normalize well ID (A1 ‚Üí A01)
                if len(well) == 2:
                    well = f"{well[0]}0{well[1]}"
                if well in wells_label and isinstance(data, dict):
                    existing_data = wells_label[well] if isinstance(wells_label[well], dict) else {
                        "label": wells_label[well]}
                    if "label" in data:
                        existing_data["label"] = data["label"]
                    existing_data.update(data)
                    wells_label[well] = existing_data

        return wells_label

    def create_parsed_resource_set(
            self, data: DataFrame, metadata: dict,
            existing_plate_layout: dict | None = None,
            medium_table: Table | None = None,
            info_table: Table | None = None,
            plate_name: str = "plate_0") -> ResourceSet:
        """
        Create a ResourceSet from parsed data with proper tagging.

        Creates one table per well (batch/sample combination) with all measurements as columns.

        :param data: Raw data DataFrame
        :param metadata: Metadata dictionary
        :param existing_plate_layout: Optional plate layout override
        :param medium_table: Optional table with medium compositions
        :param info_table: Optional table mapping wells to medium names
        :param plate_name: Name of the plate (e.g., "plate_0", "plate_1")
        :return: ResourceSet containing one table per well
        """
        self.log_info_message(f"\nüèóÔ∏è [CREATE_RESOURCE_SET] Starting for plate: {plate_name}")
        self.log_info_message(f"üìä [CREATE_RESOURCE_SET] Input data shape: {data.shape}")

        resource_set = ResourceSet()

        # Prepare medium composition mapping if tables are provided
        well_to_medium = {}
        medium_compositions = {}

        if medium_table is not None and info_table is not None:
            # Get DataFrames
            medium_df = medium_table.get_data()
            info_df = info_table.get_data()

            # Normalize well IDs in info_table (A1 -> A01, etc.)
            def normalize_well_id(well_id: str) -> str:
                if len(well_id) == 2:
                    return f"{well_id[0]}0{well_id[1]}"
                return well_id

            info_df['Well'] = info_df['Well'].apply(normalize_well_id)

            # Create well -> medium name mapping
            well_to_medium = dict(zip(info_df['Well'], info_df['Medium']))

            # Create medium name -> composition dict mapping
            for _, row in medium_df.iterrows():
                medium_name = row['Medium']
                composition = {col: row[col] for col in medium_df.columns if col != 'Medium'}
                medium_compositions[medium_name] = composition

            # Medium data prepared for tagging

        # Get parsed data (one DataFrame per filter/channel)
        parsed_data: dict[str, DataFrame] = self.parse_data(data=data, metadata=metadata)
        self.log_info_message(f"üì¶ [CREATE_RESOURCE_SET] Parsed data contains {len(parsed_data)} filters: {list(parsed_data.keys())}")

        wells_data = self.get_wells_label_description(metadata=metadata,
                                                       existing_plate_layout=existing_plate_layout)

        # Get expected wells from metadata
        expected_wells = self.get_wells(metadata)  # Cultivation + Reservoir wells
        cultivation_wells = self.get_wells_cultivation(metadata)
        self.log_info_message(f"üîç [CREATE_RESOURCE_SET] Expected wells from metadata: {len(expected_wells)} wells")
        self.log_info_message(f"üß™ [CREATE_RESOURCE_SET] Cultivation wells: {len(cultivation_wells)} wells")

        # Get all well columns (excluding time columns)
        first_filter = list(parsed_data.keys())[0] if parsed_data else None
        if not first_filter:
            self.log_warning_message("‚ö†Ô∏è [CREATE_RESOURCE_SET] No filters in parsed_data, returning empty resource_set")
            return resource_set

        first_df = parsed_data[first_filter]
        self.log_info_message(f"üìä [CREATE_RESOURCE_SET] First filter '{first_filter}' has shape: {first_df.shape}")
        self.log_info_message(f"üìã [CREATE_RESOURCE_SET] First filter columns: {list(first_df.columns)}")

        # Filter out standardized time column 'Time'
        well_columns = [col for col in first_df.columns if col != 'Time']
        self.log_info_message(f"üî¢ [CREATE_RESOURCE_SET] Well columns detected: {len(well_columns)} wells: {well_columns[:10]}...")

        # Find missing wells (expected but no data)
        wells_with_data = set(well_columns)
        expected_cultivation_set = set(cultivation_wells)
        missing_wells = expected_cultivation_set - wells_with_data

        if missing_wells:
            self.log_info_message(f"‚ö†Ô∏è {len(missing_wells)} missing wells (in metadata but no data)")

        # Create one table per well
        self.log_info_message(f"\nüîÑ [CREATE_RESOURCE_SET] Creating tables for {len(well_columns)} wells...")
        tables_created = 0

        for well in well_columns:
            # Convert well name from C01 format to C1 format (remove leading zero)
            # This ensures consistency with resource names used throughout the system
            well_clean = f"{well[0]}{int(well[1:])}" if len(well) >= 2 else well

            # Start with time column 'Time' (in hours)
            well_df = first_df[['Time']].copy()

            # Add measurement columns for this well from each filter
            for filter_name, filter_df in parsed_data.items():
                if well in filter_df.columns:
                    # Rename the well column to the filter name
                    well_df[filter_name] = filter_df[well]

            # Only create table if we have data (not all NaN)
            if not well_df.drop(columns=['Time']).isna().all().all():
                table = Table(well_df)
                table.name = well_clean  # Use clean name without leading zero

                # Get user info for tag origins
                user_id = CurrentUserService.get_current_user().id if CurrentUserService.get_current_user() else None
                origins = TagOrigins(TagOriginType.USER, user_id)

                # Add column tags for proper column identification
                # Tag Time column as index column (in hours)
                table.add_column_tag_by_name('Time', 'column_name', 'Time')
                table.add_column_tag_by_name('Time', 'unit', 'h')
                table.add_column_tag_by_name('Time', 'is_index_column', 'true')

                # Tag measurement columns as data columns
                for col in table.column_names:
                    if col != 'Time':
                        # This is a measurement column (Biomass, pH, pO2, etc.)
                        table.add_column_tag_by_name(col, 'column_name', col)
                        table.add_column_tag_by_name(col, 'is_data_column', 'true')

                # Add batch and sample tags only
                # Batch is the plate name, sample is the well identifier
                batch_tag = Tag(key='batch', value=plate_name, auto_parse=True,
                                origins=origins, is_propagable=True)
                sample_tag = Tag(key='sample', value=well_clean, auto_parse=True,  # Use clean name
                                 origins=origins, is_propagable=True)
                table.tags._tags.extend([batch_tag, sample_tag])

                # Add medium tag if medium data is available for this well
                if well in well_to_medium:
                    medium_name = well_to_medium[well]
                    medium_composition = medium_compositions.get(medium_name, {})

                    medium_tag = Tag(key='medium', value=medium_name, auto_parse=True,
                                    additional_info={'composed': medium_composition},
                                    origins=origins, is_propagable=True)
                    table.tags._tags.append(medium_tag)

                # Check if well is missing from plate_layout (only if plate_layout is provided)
                if existing_plate_layout:
                    # Try both formats: C01 and C1
                    well_normalized = well  # C01 format
                    well_short = well_clean  # C1 format

                    has_plate_layout = (well_normalized in existing_plate_layout or
                                       well_short in existing_plate_layout)

                    if not has_plate_layout:
                        # Well is missing from plate_layout - add missing_value tag
                        missing_tag = Tag(key='missing_value', value='plate_layout', auto_parse=True,
                                         origins=origins, is_propagable=True)
                        table.tags._tags.append(missing_tag)

                resource_set.add_resource(table, well_clean)  # Use clean name
                tables_created += 1
                if tables_created <= 3 or tables_created % 10 == 0:
                    self.log_info_message(f"‚úÖ [CREATE_RESOURCE_SET] Created table {tables_created}: {well_clean}")
            else:
                self.log_info_message(f"‚è≠Ô∏è [CREATE_RESOURCE_SET] Skipped well {well_clean} (all NaN)")

        self.log_info_message(f"\n‚úÖ [CREATE_RESOURCE_SET] Completed. Created {tables_created} tables in resource_set")

        # Create tables for missing wells (expected in metadata but no data in raw_data)
        wells_with_data = set(well_columns)
        expected_cultivation_set = set(cultivation_wells)
        missing_wells = expected_cultivation_set - wells_with_data

        if missing_wells:
            self.log_info_message(f"\nCreating {len(missing_wells)} empty tables for missing wells")
            user_id = CurrentUserService.get_current_user().id if CurrentUserService.get_current_user() else None
            origins = TagOrigins(TagOriginType.USER, user_id)

            for well in sorted(missing_wells):
                # Convert well name from C01 format to C1 format
                well_clean = f"{well[0]}{int(well[1:])}" if len(well) >= 2 else well

                # Create empty table with just time column
                empty_df = DataFrame({'Temps_en_h': []})
                table = Table(empty_df)
                table.name = well_clean

                # Add column tags
                table.add_column_tag_by_name('Temps_en_h', 'column_name', 'Temps')
                table.add_column_tag_by_name('Temps_en_h', 'unit', 'h')
                table.add_column_tag_by_name('Temps_en_h', 'is_index_column', 'true')

                # Add batch and sample tags
                batch_tag = Tag(key='batch', value='plate_0', auto_parse=True,
                                origins=origins, is_propagable=True)
                sample_tag = Tag(key='sample', value=well_clean, auto_parse=True,
                                 origins=origins, is_propagable=True)
                table.tags._tags.extend([batch_tag, sample_tag])

                # Add medium tag if medium data is available for this well
                if well in well_to_medium:
                    medium_name = well_to_medium[well]
                    medium_composition = medium_compositions.get(medium_name, {})

                    medium_tag = Tag(key='medium', value=medium_name, auto_parse=True,
                                    additional_info={'composed': medium_composition},
                                    origins=origins, is_propagable=True)
                    table.tags._tags.append(medium_tag)

                # Add missing_value tag for raw_data
                missing_tag = Tag(key='missing_value', value='raw_data', auto_parse=True,
                                 origins=origins, is_propagable=True)
                table.tags._tags.append(missing_tag)

                # Also check plate_layout for this missing well
                if existing_plate_layout:
                    well_normalized = well  # C01 format
                    well_short = well_clean  # C1 format

                    has_plate_layout = (well_normalized in existing_plate_layout or
                                       well_short in existing_plate_layout)

                    if not has_plate_layout:
                        # Well is also missing from plate_layout - update tag
                        table.tags._tags = [tag for tag in table.tags._tags if tag.key != 'missing_value']
                        combined_missing_tag = Tag(key='missing_value', value='raw_data, plate_layout',
                                                   auto_parse=True, origins=origins, is_propagable=True)
                        table.tags._tags.append(combined_missing_tag)

                resource_set.add_resource(table, well_clean)

        return resource_set

    def create_metadata_table(self, resource_set: ResourceSet,
                              existing_plate_layout: dict | None = None,
                              medium_table: Table | None = None,
                              info_table: Table | None = None) -> Table:
        """
        Create a metadata table for machine learning purposes.

        Combines well metadata from plate_layout or medium composition data.
        Each row represents one well (batch_sample combination).

        :param resource_set: ResourceSet containing all well tables
        :param existing_plate_layout: Optional plate layout with well metadata (legacy)
        :param medium_table: Optional table with medium compositions
        :param info_table: Optional table mapping wells to medium names
        :return: Table with metadata for ML feature extraction
        """
        import pandas as pd

        metadata_rows = []

        # If medium_table and info_table are provided, use them
        if medium_table is not None and info_table is not None:
            self.log_info_message("Creating metadata table from medium_table and info_table...")

            # Get dataframes
            medium_df = medium_table.get_data()
            info_df = info_table.get_data()

            # Normalize well identifiers in info_df to C1 format (C01 ‚Üí C1, C10 ‚Üí C10, A1 ‚Üí A1, A01 ‚Üí A1)
            def normalize_well_id(well_id):
                """Normalize well ID to format like A1, C1, C10, etc. (without leading zero)."""
                well_id = str(well_id).strip()
                if len(well_id) >= 2:
                    # Extract letter and number parts
                    letter = well_id[0]
                    number_str = well_id[1:]
                    # Convert to int and back to string to remove leading zeros
                    return f"{letter}{int(number_str)}"
                return well_id

            # Create a copy to avoid modifying the original
            info_df_normalized = info_df.copy()
            info_df_normalized['Well'] = info_df_normalized['Well'].apply(normalize_well_id)

            # Create a mapping from well to medium name
            well_to_medium = dict(zip(info_df_normalized['Well'], info_df_normalized['Medium']))

            # Identify info columns (all columns except Well and Medium)
            # Keep all info columns including string columns (compound, label, etc.)
            info_columns = [col for col in info_df_normalized.columns if col not in ['Well', 'Medium']]

            for well_name, table in resource_set.get_resources().items():
                if not isinstance(table, Table):
                    continue

                # Extract plate_name from batch tag
                batch_tags = table.tags.get_by_key('batch')
                plate_name = batch_tags[0].value if batch_tags else 'plate_0'

                # Initialize row with Series identifier
                metadata_row = {
                    'Series': f"{plate_name}_{well_name}"
                }

                # Get medium name for this well
                medium_name = well_to_medium.get(well_name)

                if medium_name:
                    # Get medium composition from medium_table
                    medium_row = medium_df[medium_df['Medium'] == medium_name]

                    if not medium_row.empty:
                        # Add all medium composition columns (except 'Medium' column)
                        for col in medium_df.columns:
                            if col != 'Medium':
                                value = medium_row.iloc[0][col]
                                metadata_row[col] = value

                # Add info columns from info_table
                info_row = info_df_normalized[info_df_normalized['Well'] == well_name]
                for col in info_columns:
                    if not info_row.empty:
                        value = info_row.iloc[0][col]
                        # Fill missing values: 0 for numeric, NaN for non-numeric
                        if pd.isna(value) or value == '':
                            # Try to infer if column is numeric
                            try:
                                float_val = float(value)
                                metadata_row[col] = 0
                            except (ValueError, TypeError):
                                metadata_row[col] = np.nan
                        else:
                            # Try to convert to numeric, else keep as is
                            try:
                                metadata_row[col] = float(value)
                            except (ValueError, TypeError):
                                metadata_row[col] = value
                    else:
                        # No info for this well: fill missing
                        # Try to infer if column is numeric from all values in info_df
                        col_vals = info_df_normalized[col].dropna().astype(str)
                        is_numeric = False
                        for v in col_vals:
                            try:
                                float(v)
                                is_numeric = True
                                break
                            except ValueError:
                                continue
                        metadata_row[col] = 0 if is_numeric else np.nan

                metadata_rows.append(metadata_row)

        else:
            # Legacy mode: use plate_layout
            self.log_info_message("Creating metadata table from plate_layout (legacy mode)...")

            for well_name, table in resource_set.get_resources().items():
                if not isinstance(table, Table):
                    continue

                # Extract plate_name from batch tag
                batch_tags = table.tags.get_by_key('batch')
                plate_name = batch_tags[0].value if batch_tags else 'plate_0'

                # Initialize row with Series identifier
                metadata_row = {
                    'Series': f"{plate_name}_{well_name}"
                }

                # Add plate_layout metadata if available
                if existing_plate_layout:
                    # well_name is already in C1 format from resource_set keys
                    # Try both formats for backward compatibility
                    well_c1 = well_name  # Already C1 format (e.g., "C1")
                    well_c01 = f"{well_name[0]}{int(well_name[1:]):02d}"  # C01 format (e.g., "C01")

                    plate_data = None
                    if well_c1 in existing_plate_layout:
                        plate_data = existing_plate_layout[well_c1]
                    elif well_c01 in existing_plate_layout:
                        plate_data = existing_plate_layout[well_c01]

                    if plate_data and isinstance(plate_data, dict):
                        for key, value in plate_data.items():
                            # Add plate_layout metadata
                            metadata_row[f"plate_{key}"] = value

                metadata_rows.append(metadata_row)

        # Create DataFrame
        if not metadata_rows:
            # Return empty table if no data
            metadata_df = pd.DataFrame()
        else:
            metadata_df = pd.DataFrame(metadata_rows)

            # Get list of medium composition columns (from medium_table if provided)
            medium_columns = set()
            if medium_table is not None:
                medium_df = medium_table.get_data()
                # Always use 'Medium' as standardized column name
                medium_columns = set(medium_df.columns) - {'Medium'}

            # Get list of info columns (from info_table if provided)
            # These should be preserved even if they contain string values
            info_columns_set = set()
            if info_table is not None:
                info_df = info_table.get_data()
                info_columns_set = set(info_df.columns) - {'Well', 'Medium'}

            # Remove columns that only contain NaN (but keep medium and info columns)
            cols_to_remove = []
            for col in metadata_df.columns:
                if col == 'Series':
                    continue

                # Keep medium composition columns even if all NaN or all 0
                if col in medium_columns:
                    continue

                # Keep info columns (including string columns like compound, label, etc.)
                if col in info_columns_set:
                    continue

                # Check if all values are NaN
                if metadata_df[col].isna().all():
                    cols_to_remove.append(col)
                    continue

                # For other columns (legacy plate_layout), check if all non-NaN values are 0
                try:
                    numeric_col = pd.to_numeric(metadata_df[col], errors='coerce')
                    non_nan_values = numeric_col.dropna()
                    if len(non_nan_values) > 0 and (non_nan_values == 0).all():
                        cols_to_remove.append(col)
                except Exception:
                    pass

            # Drop identified columns
            if cols_to_remove:
                metadata_df = metadata_df.drop(columns=cols_to_remove)

        # Create Table
        metadata_table = Table(metadata_df)
        metadata_table.name = "Metadata Table for ML"

        return metadata_table

    def run(self, params: ConfigParams, inputs: TaskInputs) -> TaskOutputs:
        """
        Execute the BiolectorXT data loading and processing.

        :param params: Task configuration parameters
        :param inputs: Task inputs
        :return: Task outputs
        """

        # Get medium table (unique across all plates)
        medium_table: Table = inputs.get('medium_table')
        self.log_info_message(f"üîç [DEBUG] medium_table from named port: {medium_table is not None} (type: {type(medium_table).__name__ if medium_table else 'None'})")

        # Get all dynamic inputs (plates as ResourceList)
        plates_resource_list: ResourceList = inputs.get('source')
        self.log_info_message(f"üîç [DEBUG] plates_resource_list type: {type(plates_resource_list).__name__}")

        if not isinstance(plates_resource_list, ResourceList):
            raise Exception(f"Expected ResourceList for 'source' input, got {type(plates_resource_list)}")

        # Get the actual list of resources from ResourceList
        plates_list = plates_resource_list.get_resources()

        # Filter out None values (can happen with optional dynamic inputs)
        plates_list = [plate for plate in plates_list if plate is not None]

        num_plates = len(plates_list)
        self.log_info_message(f"üîç [DEBUG] Total items in plates_list (after filtering None): {num_plates}")

        # Log each item in plates_list
        for i, item in enumerate(plates_list):
            self.log_info_message(f"üîç [DEBUG] plates_list[{i}]: {type(item).__name__}")

        # Check if first element is medium_table (Table) instead of ResourceSet
        # Only use it if medium_table was not already provided via named port
        start_idx = 0
        if medium_table is None and num_plates > 0 and isinstance(plates_list[0], Table):
            # Legacy mode: medium_table passed as first dynamic input
            medium_table = plates_list[0]
            start_idx = 1
            self.log_info_message(f"‚úÖ [DEBUG] Found medium_table as first dynamic input, start_idx = {start_idx}")
        elif medium_table is not None:
            self.log_info_message(f"‚úÖ [DEBUG] Using medium_table from named input port, start_idx = {start_idx}")

        actual_plates = plates_list[start_idx:]
        num_actual_plates = len(actual_plates)
        self.log_info_message(f"üìä [DEBUG] actual_plates count: {num_actual_plates}")
        for i, plate in enumerate(actual_plates):
            self.log_info_message(f"üìä [DEBUG] actual_plates[{i}]: {type(plate).__name__}")

        self.log_info_message(f"Processing {num_actual_plates} plate(s)")

        # Get plate names from config
        plate_names: list[str] = params.get_value('plate_names')
        self.log_info_message(f"üè∑Ô∏è [DEBUG] plate_names from config: {plate_names}")

        # Validate plate names
        if plate_names and len(plate_names) > 0:
            # User provided plate names - validate count
            self.log_info_message(f"üîç [DEBUG] Validating: {len(plate_names)} names vs {num_actual_plates} plates")
            if len(plate_names) != num_actual_plates:
                raise Exception(
                    f"Number of plate names ({len(plate_names)}) does not match number of input plates ({num_actual_plates}). "
                    f"Please provide either no names (for default names) or exactly {num_actual_plates} names."
                )
            self.log_info_message(f"Using custom plate names: {plate_names}")
        else:
            # Use default plate names
            plate_names = [f"plate_{i}" for i in range(num_actual_plates)]
            self.log_info_message(f"Using default plate names: {plate_names}")

        # Process each plate

        # Initialize combined outputs
        all_resource_sets = []
        all_cultivation_labels = set()
        all_medium_info_wells = set()
        all_raw_data_wells = set()
        all_metadata_dfs = []

        # Process each plate
        for plate_idx, plate_resource_set in enumerate(actual_plates):
            plate_name = plate_names[plate_idx]
            self.log_info_message(f"\n{'=' * 80}")
            self.log_info_message(f"PROCESSING {plate_name.upper()}")
            self.log_info_message(f"{'=' * 80}")

            # Extract resources from ResourceSet
            if not isinstance(plate_resource_set, ResourceSet):
                raise Exception(
                    f"Plate {plate_idx} must be a ResourceSet containing 'raw_data', 'folder_metadata', and 'info_table'. "
                    f"Got {type(plate_resource_set)}"
                )

            # Get inputs for this plate from the ResourceSet
            raw_data: Table = plate_resource_set.get_resource('raw_data')
            folder_metadata: Folder = plate_resource_set.get_resource('folder_metadata')

            # info_table is optional
            info_table: Table = None
            if plate_resource_set.resource_exists('info_table'):
                info_table = plate_resource_set.get_resource('info_table')

            if raw_data is None or folder_metadata is None:
                raise Exception(
                    f"Plate {plate_idx} ResourceSet must contain 'raw_data' (Table) and "
                    f"'folder_metadata' (Folder). 'info_table' is optional. "
                    f"Found: {list(plate_resource_set.get_resources().keys())}"
                )

            # Load metadata file

            # Load metadata
            metadata: dict = None
            for file_name in os.listdir(folder_metadata.path):
                if file_name.endswith('BXT.json'):
                    file_path = os.path.join(folder_metadata.path, file_name)
                    try:
                        with open(file_path, 'r', encoding='UTF-8') as json_file:
                            metadata = json.load(json_file)

                    except Exception as e:
                        raise Exception(f"Error while reading the metadata file {file_name}: {e}")

            if metadata is None:
                raise Exception(
                    f"No metadata file found in the provided folder for {plate_name}. "
                    "The folder must contain a file that ends with 'BXT.json'"
                )

            # Parse data for this plate

            # Create parsed resource set for this plate
            self.log_info_message(f"Parsing BiolectorXT data for {plate_name}...")
            resource_set = self.create_parsed_resource_set(
                data=raw_data.get_data(),
                metadata=metadata,
                existing_plate_layout=None,
                medium_table=medium_table,
                info_table=info_table,
                plate_name=plate_name
            )

            # Copy download tags from raw data
            resource_set.tags.add_tags(raw_data.tags.get_by_key(DOWNLOAD_TAG_KEY))

            self.log_success_message(f"Created {len(resource_set.get_resources())} parsed tables for {plate_name}")

            # Collect for combined outputs
            all_resource_sets.append(resource_set)

            # Gather sets of wells for Venn diagram with plate prefix
            cultivation_labels_set = set(self.get_wells_cultivation(metadata))
            # Add plate prefix to cultivation labels (convert C01 to C1 format)
            for well in cultivation_labels_set:
                well_c1 = f"{well[0]}{int(well[1:])}" if len(well) > 1 else well
                all_cultivation_labels.add(f"({plate_name}_{well_c1})")

            # Wells with medium info (from info_table if provided)
            medium_info_wells = set()
            if info_table is not None:
                info_df = info_table.get_data()
                # Normalize well IDs to C1 format (remove leading zeros)
                def normalize_well_id(well_id):
                    """Convert to C1 format: C01 ‚Üí C1, C10 ‚Üí C10, A1 ‚Üí A1"""
                    if isinstance(well_id, str) and len(well_id) >= 2:
                        letter = well_id[0]
                        number_str = well_id[1:]
                        return f"{letter}{int(number_str)}"
                    return well_id
                info_df_normalized = info_df.copy()
                info_df_normalized['Well'] = info_df_normalized['Well'].apply(normalize_well_id)
                medium_info_wells = set(info_df_normalized['Well'].unique())
            # Add plate prefix to medium info wells (convert C01 to C1 format)
            for well in medium_info_wells:
                well_c1 = f"{well[0]}{int(well[1:])}" if len(well) > 1 else well
                all_medium_info_wells.add(f"({plate_name}_{well_c1})")

            # Wells with raw_data (already in C1 format from resource_set keys)
            raw_data_wells = set(resource_set.get_resources().keys())
            # Add plate prefix to raw data wells (wells are already in C1 format)
            for well in raw_data_wells:
                all_raw_data_wells.add(f"({plate_name}_{well})")

            # Create metadata table for this plate and collect it
            plate_metadata_table = self.create_metadata_table(
                resource_set,
                None,
                medium_table,
                info_table
            )
            all_metadata_dfs.append(plate_metadata_table.get_data())

        # Combine all resource sets with plate prefixes
        combined_resource_set = ResourceSet()
        for plate_idx, resource_set in enumerate(all_resource_sets):
            plate_name = plate_names[plate_idx]
            for well_name, table in resource_set.get_resources().items():
                # Remove leading zero from well number (C01 -> C1, C10 stays C10)
                well_name_short = well_name[0] + str(int(well_name[1:]))
                # Format: plate_0_C1 or custom_name_C1
                combined_well_name = f"{plate_name}_{well_name_short}"
                # Update the table name to match
                table.name = combined_well_name
                combined_resource_set.add_resource(table, combined_well_name)

        self.log_success_message(f"Combined total: {len(combined_resource_set.get_resources())} parsed tables from {num_actual_plates} plate(s)")

        # Combine all metadata DataFrames
        if all_metadata_dfs:
            combined_metadata_df = pd.concat(all_metadata_dfs, ignore_index=True)
            metadata_table = Table(combined_metadata_df)
            metadata_table.name = "Metadata table"
            self.log_success_message(f"Combined metadata table created with {len(combined_metadata_df)} wells")
        else:
            metadata_table = Table(pd.DataFrame({'Series': []}))
            metadata_table.name = "Metadata table"

        # Create Venn diagram for well data availability using combined data
        well_sets = {
            'cultivation_labels': all_cultivation_labels,
            'medium_info': all_medium_info_wells,
            'raw_data': all_raw_data_wells
        }

        # Create Venn diagram
        venn_diagram = None
        if any(len(s) > 0 for s in well_sets.values()):
            fig = create_venn_diagram_wells(well_sets)
            venn_diagram = PlotlyResource(fig)
            venn_diagram.name = "BiolectorXT Well Data Availability"

        # Clean NaN values in metadata table numeric columns
        metadata_df = metadata_table.get_data().copy()

        # Try to convert columns to numeric only if they are actually numeric
        # Skip identifier columns and columns that are primarily string values
        for col in metadata_df.columns:
            if col == 'Series':
                continue

            # Skip columns that are already numeric
            if pd.api.types.is_numeric_dtype(metadata_df[col]):
                continue

            # Try converting to numeric
            converted_col = pd.to_numeric(metadata_df[col], errors='coerce')
            # Only keep the conversion if at least 50% of values are valid numbers
            # This preserves string columns (compound, label, etc.) which would have 0% valid numbers
            valid_ratio = converted_col.notna().sum() / len(converted_col) if len(converted_col) > 0 else 0
            if valid_ratio >= 0.5:
                metadata_df[col] = converted_col

        numeric_cols_meta = metadata_df.select_dtypes(include=[np.number]).columns.tolist()

        if numeric_cols_meta:
            nan_counts_meta = metadata_df[numeric_cols_meta].isna().sum()
            total_nans_meta = nan_counts_meta.sum()

            if total_nans_meta > 0:
                metadata_df[numeric_cols_meta] = metadata_df[numeric_cols_meta].fillna(0)

        # Update metadata table with cleaned data
        metadata_table = Table(metadata_df)
        metadata_table.name = "Metadata table"

        # Prepare outputs
        outputs = {
            'resource_set': combined_resource_set,
            'venn_diagram': venn_diagram,
            'metadata_table': metadata_table
        }

        # Add medium_table to outputs if provided as input
        if medium_table is not None:
            # Replace NaN values in numeric columns with 0
            medium_df = medium_table.get_data().copy()

            # Try to convert columns to numeric only if they are actually numeric
            # Skip identifier columns and columns that are primarily string values
            for col in medium_df.columns:
                if col == 'Medium':
                    continue

                # Skip columns that are already numeric
                if pd.api.types.is_numeric_dtype(medium_df[col]):
                    continue

                # Try converting to numeric
                converted_col = pd.to_numeric(medium_df[col], errors='coerce')
                # Only keep the conversion if at least 50% of values are valid numbers
                # This preserves string columns which would have 0% valid numbers
                valid_ratio = converted_col.notna().sum() / len(converted_col) if len(converted_col) > 0 else 0
                if valid_ratio >= 0.5:
                    medium_df[col] = converted_col

            numeric_cols = medium_df.select_dtypes(include=[np.number]).columns.tolist()

            if numeric_cols:
                nan_counts = medium_df[numeric_cols].isna().sum()
                total_nans = nan_counts.sum()

                if total_nans > 0:
                    medium_df[numeric_cols] = medium_df[numeric_cols].fillna(0)

                    # Update the table with cleaned data
                    medium_table = Table(medium_df)
                    medium_table.name = "Medium table"

            outputs['medium_table'] = medium_table

        # Return outputs
        return outputs
