from gws_core import (InputSpec, OutputSpec, InputSpecs, OutputSpecs, Table, Tag,
                      TypingStyle, ResourceSet, Task, task_decorator, ConfigSpecs, ConfigParams,
                      StrParam, IntParam)
from typing import Dict, Any, List, Tuple
from scipy.interpolate import (
    UnivariateSpline,
    PchipInterpolator,
    Akima1DInterpolator,
    CubicSpline,
    interp1d,
)
import pandas as pd
import numpy as np


@task_decorator("CellCultureSubsampling",
                human_name="Cell Culture Time Series Subsampling",
                short_description="Subsample fermentation time series data by combining real and interpolated values",
                style=TypingStyle.material_icon(material_icon_name="line_axis", background_color="#17a2b8"))
class CellCultureSubsampling(Task):
    """
    [Generated by Task Expert Agent]

    Subsample fermentation time series data by combining real and interpolated values.

    ## Overview
    This task creates a combined dataset where:
    - Interpolated columns use only interpolated values
    - Non-interpolated columns use only real measured values
    - Time column contains both real and interpolated time points

    ## Purpose
    - **Smart Data Combination**: Use interpolation only where it makes sense
    - **Preserve Real Data**: Keep original measurements for columns with sparse data
    - **Enable Comparison**: Create uniform datasets while respecting data quality
    - **Prepare for Analysis**: Output ready for ML and statistical analysis

    ## Interpolation Methods

    ### Shape-Preserving Methods (Recommended for Biological Data)

    #### `makima` (Default) ★ RECOMMENDED
    - **Modified Akima Interpolation**: Enhanced shape-preserving method
    - **Best For**: Biological time series with varying growth phases
    - **Advantages**:
      - Avoids overshoot in steep regions (pH, biomass)
      - Natural-looking curves following data trends
      - Robust to outliers
      - Smooth transitions between regions
    - **Use When**: General fermentation data analysis

    #### `pchip`
    - **Piecewise Cubic Hermite Interpolating Polynomial**
    - **Best For**: Data with local extrema that must be preserved
    - **Advantages**:
      - Guarantees monotonicity (no artificial peaks/valleys)
      - Shape-preserving in regions of constant slope
      - Good for concentration measurements
    - **Use When**: Substrate consumption, product formation curves

    #### `akima`
    - **Original Akima Spline**: Classic shape-preserving method
    - **Best For**: Smooth biological responses
    - **Advantages**:
      - Very smooth curves
      - Less sensitive to outliers than cubic splines
      - Natural interpolation
    - **Use When**: Temperature, pressure, flow rate data

    ### Polynomial Methods

    #### `linear`
    - **Linear Interpolation**: Straight lines between points
    - **Best For**: High-frequency data, preliminary analysis
    - **Advantages**: Fast, simple, no overshoot
    - **Disadvantages**: Angular appearance, not smooth
    - **Use When**: Quick checks, high sampling rate data

    #### `quadratic`
    - **Quadratic Spline**: Piecewise second-degree polynomials
    - **Best For**: Slowly varying parameters
    - **Advantages**: Smoother than linear, faster than cubic
    - **Use When**: Steady-state measurements

    #### `cubic`
    - **Cubic Spline**: Piecewise third-degree polynomials
    - **Best For**: Very smooth data
    - **Advantages**: Smooth, twice differentiable
    - **Disadvantages**: Can overshoot between points
    - **Use When**: High-quality data with minimal noise

    ### Advanced Spline Methods

    #### `cubic_spline`
    - **Natural Cubic Spline**: Global smoothing with boundary conditions
    - **Best For**: Complete curve fitting
    - **Advantages**: Global smoothness, natural boundary behavior
    - **Use When**: Need derivatives, mathematical modeling

    #### `univariate_spline` or `spline`
    - **Adaptive Order B-Spline**: Automatic order selection
    - **Best For**: Complex curves with varying smoothness
    - **Advantages**: Adaptive, handles varied data patterns
    - **Configuration**: `spline_order` (1-5, default=3)
    - **Use When**: Unknown data behavior, exploratory analysis

    #### `nearest`
    - **Nearest Neighbor**: Step-wise interpolation
    - **Best For**: Categorical data, control parameters
    - **Use When**: Binary states, control settings over time

    ## Grid Strategies

    ### `global_auto` (Default) ★ RECOMMENDED
    - Creates ONE common time grid for ALL samples
    - **Advantages**:
      - Direct sample comparison at same time points
      - Optimal for plotting multiple samples together
      - Consistent analysis across dataset
    - **Algorithm**:
      1. Finds global min/max time across all samples
      2. Calculates median time step from all samples
      3. Generates uniform grid spanning entire range
    - **Use When**: Comparing multiple fermentations

    ### `per_file`
    - Creates INDIVIDUAL time grid for EACH sample
    - **Advantages**:
      - Preserves each sample's specific time range
      - Better for samples with very different durations
      - Optimal grid density per sample
    - **Use When**: Samples have vastly different durations

    ### `reference`
    - Uses time grid from ONE specific sample as template
    - **Configuration**: `reference_index` (0-based index of reference sample)
    - **Advantages**:
      - Forces all samples to exact same time points
      - Perfect alignment for direct arithmetic operations
    - **Use When**:
      - Need exact alignment for calculations
      - One sample is the "gold standard"

    ## Configuration Parameters

    ### Essential Parameters

    #### `method` (String)
    - **Default**: `"makima"`
    - **Options**: See Interpolation Methods section
    - **Impact**: Determines curve shape and smoothness
    - **Recommendation**: Start with `makima`, try `pchip` if issues

    #### `grid_strategy` (String)
    - **Default**: `"global_auto"`
    - **Options**: `"global_auto"`, `"per_file"`, `"reference"`
    - **Impact**: Determines time alignment across samples
    - **Recommendation**: Use default unless specific needs

    #### `n_points` (Integer)
    - **Default**: 500
    - **Range**: 10 to 20,000
    - **Impact**:
      - Higher: Smoother curves, larger files, slower processing
      - Lower: Faster processing, may miss details
    - **Auto Mode**: If not set, calculated from data characteristics
    - **Recommendation**:
      - 100-300: Quick previews
      - 500-1000: Standard analysis
      - 1000-5000: Publication quality
      - 5000+: Detailed mathematical analysis

    ### Advanced Parameters

    #### `spline_order` (Integer)
    - **Default**: 3
    - **Range**: 1 to 5
    - **Only For**: `univariate_spline` / `spline` method
    - **Values**:
      - 1: Linear (same as linear method)
      - 2: Quadratic
      - 3: Cubic (smooth, flexible) ★
      - 4-5: Very smooth (may oscillate)

    #### `edge_strategy` (String)
    - **Default**: `"nearest"`
    - **Options**:
      - `"nearest"`: Extend with nearest value (flat)
      - `"linear"`: Linear extrapolation
      - `"nan"`: Fill with NaN (missing)
    - **Impact**: How to handle time points outside original range
    - **Recommendation**: Use default (`nearest`) for safety

    #### `reference_index` (Integer)
    - **Default**: 0
    - **Only For**: `grid_strategy="reference"`
    - **Range**: 0 to (number_of_samples - 1)
    - **Impact**: Which sample's time grid to use as template

    ## Input Requirements

    ### ResourceSet Structure
    - **Source**: CellCultureLoadData or Filter task output
    - **Contents**: Table resources with time series data
    - **Required Column**: `"Temps de culture (h)"` (culture time in hours)
    - **Required Tags**:
      - `batch`: Experiment identifier (preserved in output)
      - `sample`: Sample identifier (preserved in output)
      - Column tag `is_index_column='true'` on time column

    ### Data Quality Requirements
    - **Numeric Data**: All measurement columns must be numeric
    - **Decimal Format**: Commas automatically converted to dots
    - **Time Values**: Must be finite (no NaN/Inf in time column)
    - **Minimum Points**: At least 2 time points per sample
    - **Sorting**: Time column will be sorted automatically

    ## Processing Steps

    1. **Data Loading**: Extracts Tables from input ResourceSet
    2. **Decimal Normalization**: Converts commas to dots in numeric columns
    3. **Time Grid Generation**: Creates interpolation grid(s) based on strategy
    4. **Quality Checks**: Validates sufficient data points, handles edge cases
    5. **Interpolation**: Applies selected method to each numeric column
    6. **Tag Preservation**: Copies all tags from original to interpolated Tables
    7. **Metadata**: Adds interpolation method tag for traceability
    8. **Output Assembly**: Combines all interpolated Tables into ResourceSet

    ## Output Structure

    ### interpolated_resource_set (ResourceSet)
    Contains one Table per input sample with:

    #### Columns
    - `Temps de culture (h)`: Uniform time grid
    - All measurement columns: Interpolated values
    - Metadata columns: Preserved as-is (ESSAI, FERMENTEUR, MILIEU)

    #### Tags (Preserved)
    - `batch`: Original experiment ID
    - `sample`: Original sample ID
    - `medium`: Original medium name
    - `missing_value`: Original missing data info
    - `interpolation_method`: NEW - Records method used

    #### Column Tags (Preserved)
    - `is_index_column='true'`: On time column
    - `is_data_column='true'`: On measurement columns
    - `unit`: Units of measurement

    #### Properties
    - Same number of rows across all samples (if global_auto)
    - Aligned time points for easy comparison
    - Smooth, continuous curves
    - Ready for plotting and analysis

    ## Use Cases

    ### 1. Multi-Sample Comparison
    ```
    Filter → Interpolate (global_auto, makima) →
    Plot all samples on same axes
    ```

    ### 2. Growth Rate Calculation
    ```
    Interpolate (cubic_spline) →
    Calculate derivatives → Analyze growth kinetics
    ```

    ### 3. Gap Filling
    ```
    Data with missing points →
    Interpolate (pchip) → Complete time series
    ```

    ### 4. Noise Reduction
    ```
    Noisy measurements →
    Interpolate (makima, fewer points) → Smoother curves
    ```

    ### 5. Machine Learning Prep
    ```
    Interpolate (global_auto, 200 points) →
    Fixed-length feature vectors → ML model
    ```

    ## Example Workflows

    ### Standard Analysis Pipeline
    ```
    CellCultureLoadData
      ↓ (50 samples with irregular sampling)
    Filter (select 10 samples)
      ↓
    Interpolate (method=makima, grid_strategy=global_auto, n_points=500)
      ↓
    [10 samples, each with 500 uniform time points]
      ↓
    Visualization / Statistical Analysis
    ```

    ### High-Quality Publication
    ```
    Filter (select best samples)
      ↓
    Interpolate (method=pchip, n_points=2000)
      ↓
    Export smooth, high-resolution curves
    ```

    ### Quick Preview
    ```
    Interpolate (method=linear, n_points=50)
      ↓
    Fast visualization for QC
    ```

    ## Performance Considerations

    - **Speed**: linear > nearest > quadratic > cubic > akima > pchip > cubic_spline > univariate_spline
    - **Memory**: Proportional to (n_samples × n_points × n_columns)
    - **Quality**: Shape-preserving methods generally best for biological data
    - **Recommendations**:
      - < 10 samples: Any method, high n_points OK
      - 10-100 samples: Use makima/pchip, n_points=500-1000
      - > 100 samples: Consider linear or lower n_points

    ## Error Handling

    ### Automatic Fallbacks
    - **Too Few Points**: Skips sample with warning
    - **Non-numeric Data**: Preserves original values
    - **NaN in Time**: Filters out invalid rows
    - **Method Failure**: Falls back to linear interpolation
    - **Spline Order**: Reduces order if insufficient points

    ### Common Issues

    | Issue | Cause | Solution |
    |-------|-------|----------|
    | Oscillations | Cubic with sparse data | Use pchip or makima |
    | Angular curves | Linear method | Switch to makima/pchip |
    | Overshoot | Cubic spline | Use shape-preserving method |
    | Slow processing | Too many points | Reduce n_points to 500-1000 |
    | Missing columns | Time column not found | Check column name exactly |

    ## Best Practices

    1. **Start Simple**: Begin with default settings (makima, global_auto)
    2. **Visual Check**: Plot original + interpolated to verify quality
    3. **Match Purpose**: Choose method based on analysis goal
    4. **Document Choice**: Interpolation method affects results
    5. **Validate**: Check that interpolation preserves key features
    6. **Right Resolution**: More points ≠ better (avoid over-sampling)

    ## Scientific Considerations

    ### For Growth Curves
    - Use `makima` or `pchip` (preserve exponential growth)
    - Avoid cubic spline (may create artificial inflection points)

    ### For Substrate/Product
    - Use `pchip` (preserves monotonic consumption/production)
    - Avoid methods that create overshoots

    ### For Environmental
    - Use `akima` (smooth physical parameter changes)
    - Linear OK if high-frequency logging

    ### For Derivatives
    - Use `cubic_spline` (smooth second derivative)
    - Higher n_points for accurate derivatives

    ## Integration with Dashboard

    The Cell Culture dashboard provides:
    1. **Interactive Selection**: Choose method from dropdown
    2. **Live Preview**: See interpolation results immediately
    3. **Comparison**: View original vs interpolated side-by-side
    4. **Export**: Download interpolated data as CSV

    ## Notes

    - Time column name is hardcoded: `"Temps de culture (h)"`
    - All numeric columns are interpolated (metadata columns preserved)
    - Original data is never modified (creates new Tables)
    - Interpolation tag added for traceability
    - Compatible with all Cell Culture workflow tasks
    - Designed for biological time-series (fermentation focus)
    """

    SUPPORTED_METHODS = {
        "linear",               # np.interp
        "nearest",              # interp1d(kind='nearest')
        "quadratic",            # interp1d(kind='quadratic')
        "cubic",                # interp1d(kind='cubic')
        "pchip",                # PchipInterpolator (shape-preserving)
        "akima",                # Akima1DInterpolator
        "makima",               # MakimaInterpolator (shape-preserving)
        "cubic_spline",         # CubicSpline
        "univariate_spline",    # UnivariateSpline (adaptive order)
        "spline",               # alias -> univariate_spline
    }

    input_specs: InputSpecs = InputSpecs({
        'resource_set': InputSpec(ResourceSet,
                                  human_name="Input ResourceSet to interpolate",
                                  short_description="ResourceSet containing cell culture time series data",
                                  optional=False),
    })

    output_specs: OutputSpecs = OutputSpecs({
        'subsampled_resource_set': OutputSpec(ResourceSet,
                                              human_name="Subsampled ResourceSet",
                                              short_description="ResourceSet with combined real and interpolated data")
    })

    config_specs = ConfigSpecs({
        'time_column': StrParam(
            human_name="Time column name",
            short_description="Name of the time column",
            default_value="Temps de culture (h)"
        ),
        'batch_column': StrParam(
            human_name="Batch column name",
            short_description="Name of the batch column to remove from output",
            default_value="ESSAI",
            optional=True
        ),
        'sample_column': StrParam(
            human_name="Sample column name",
            short_description="Name of the sample column to remove from output",
            default_value="FERMENTEUR",
            optional=True
        ),
        'method': StrParam(
            human_name="Interpolation method",
            short_description="Method: linear, nearest, quadratic, cubic, pchip, akima, makima, cubic_spline, univariate_spline, spline",
            default_value="makima",
            allowed_values=list(SUPPORTED_METHODS)
        ),
        'grid_strategy': StrParam(
            human_name="Grid strategy",
            short_description="Strategy for time grid generation",
            default_value="global_auto",
            allowed_values=["global_auto", "per_file", "reference"]
        ),
        'n_points': IntParam(
            human_name="Number of points",
            short_description="Number of points in interpolation grid (auto if not set)",
            default_value=500,
            min_value=10,
            max_value=20000,
            optional=True
        ),
        'spline_order': IntParam(
            human_name="Spline order",
            short_description="Order of spline interpolation for univariate_spline method (1-5)",
            default_value=3,
            min_value=1,
            max_value=5
        ),
        'edge_strategy': StrParam(
            human_name="Edge handling strategy",
            short_description="How to handle data beyond original time range",
            default_value="nan",
            allowed_values=["nearest", "linear", "nan"]
        ),
        'reference_index': IntParam(
            human_name="Reference resource index",
            short_description="Index of resource to use as reference for grid (when grid_strategy='reference')",
            default_value=0,
            min_value=0
        ),
        'min_values_threshold': IntParam(
            human_name="Minimum values threshold",
            short_description="Interpolate only columns with at least this many non-NaN values. If None, interpolate all columns.",
            default_value=None,
            min_value=1,
            optional=True
        )
    })

    def convert_commas_to_dots(self, df: pd.DataFrame) -> pd.DataFrame:
        """Convert comma decimal separators to dots and coerce to numeric where possible"""
        out = df.copy()
        for col in out.columns:
            if out[col].dtype == object:
                # replace decimal comma by dot
                ser = out[col].astype(str).str.replace(",", ".", regex=False).replace("nan", np.nan)
                # try numeric coercion
                ser_num = pd.to_numeric(ser, errors="coerce")
                # keep numeric series if it has at least some finite values, else keep original text
                if np.isfinite(ser_num.astype(float)).sum() > 0:
                    out[col] = ser_num
                else:
                    out[col] = ser  # leave as cleaned text
        return out

    def auto_n_points_from_time_list(self, t_list: List[np.ndarray], max_points=20000, min_points=100) -> int:
        """Auto-calculate number of points based on time span and median time step"""
        dts = []
        for t in t_list:
            t = np.asarray(t, dtype=float)
            t = t[np.isfinite(t)]
            if t.size >= 2:
                t_sorted = np.sort(t)
                dt = np.diff(t_sorted)
                dt = dt[np.isfinite(dt) & (dt > 0)]
                if dt.size:
                    dts.append(np.median(dt))
        if not dts:
            return 1000
        median_dt = float(np.median(dts))
        tmins = [np.nanmin(tt) for tt in t_list if np.size(tt) > 0]
        tmaxs = [np.nanmax(tt) for tt in t_list if np.size(tt) > 0]
        if not tmins or not tmaxs or median_dt <= 0:
            return 1000
        span = float(np.nanmax(tmaxs) - np.nanmin(tmins))
        est = int(np.ceil(span / median_dt))
        return max(min_points, min(est, max_points))

    def prepare_dataframe(self, df: pd.DataFrame, time_col: str) -> pd.DataFrame:
        """Prepare DataFrame for interpolation"""
        df = self.convert_commas_to_dots(df)
        if time_col not in df.columns:
            raise KeyError(f"'{time_col}' column not found in data.")
        df[time_col] = pd.to_numeric(df[time_col], errors="coerce")
        df = df.dropna(subset=[time_col]).sort_values(time_col)
        return df

    def build_global_grid(self, dfs: List[pd.DataFrame], time_col: str, n_points: int = None) -> np.ndarray:
        """Build a global time grid covering all data"""
        t_arrays = [d[time_col].to_numpy() for d in dfs]
        tmin = min(float(np.nanmin(t)) for t in t_arrays if t.size)
        tmax = max(float(np.nanmax(t)) for t in t_arrays if t.size)
        if not np.isfinite(tmin) or not np.isfinite(tmax) or tmin == tmax:
            raise ValueError("Invalid or no time range found.")
        if n_points is None:
            n_points = self.auto_n_points_from_time_list(t_arrays)
        return np.linspace(tmin, tmax, int(n_points))

    def _dedup_time_average(self, t_valid: np.ndarray, y_valid: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Remove duplicate time points by averaging y values"""
        t_unique, inv = np.unique(t_valid, return_inverse=True)
        if t_unique.size == t_valid.size:
            return t_valid, y_valid
        y_accum = np.zeros_like(t_unique, dtype=float)
        counts = np.zeros_like(t_unique, dtype=int)
        for i, yi in zip(inv, y_valid):
            y_accum[i] += yi
            counts[i] += 1
        y_avg = y_accum / np.maximum(counts, 1)
        return t_unique, y_avg

    def _core_interpolate(
        self,
        t_valid: np.ndarray,
        y_valid: np.ndarray,
        t_new: np.ndarray,
        method: str,
        spline_order: int,
    ) -> np.ndarray:
        """
        Returns interpolated values at t_new (WITHOUT edge extrapolation policy).
        Expects t_valid strictly increasing and deduplicated.
        """
        m = int(len(t_valid))
        if m < 2:
            return np.full_like(t_new, y_valid[0] if m == 1 else np.nan, dtype=float)

        method = method.lower()
        if method not in self.SUPPORTED_METHODS:
            raise ValueError(
                f"Unknown method='{method}'. Supported: {sorted(self.SUPPORTED_METHODS)}"
            )
        if method == "spline":
            method = "univariate_spline"

        # Fast linear interpolation
        if method == "linear":
            return np.interp(t_new, t_valid, y_valid)

        # Nearest neighbor
        if method == "nearest":
            f = interp1d(t_valid, y_valid, kind="nearest", bounds_error=False,
                         fill_value=(y_valid[0], y_valid[-1]))
            return f(t_new)

        # Quadratic and cubic from scipy.interpolate.interp1d
        if method in ("quadratic", "cubic"):
            f = interp1d(t_valid, y_valid, kind=method, bounds_error=False,
                         fill_value=(y_valid[0], y_valid[-1]))
            return f(t_new)

        # PCHIP - shape-preserving
        if method == "pchip":
            f = PchipInterpolator(t_valid, y_valid, axis=0)
            return f(t_new)

        # Akima - shape-preserving and smooth
        if method == "akima":
            f = Akima1DInterpolator(t_valid, y_valid, method='akima', axis=0)
            return f(t_new)

        # Makima
        if method == "makima":
            f = Akima1DInterpolator(t_valid, y_valid, method='makima', axis=0)
            return f(t_new)

        # Natural cubic spline
        if method == "cubic_spline":
            f = CubicSpline(t_valid, y_valid)
            return f(t_new)

        # Univariate spline with adaptive order and fallback
        if method == "univariate_spline":
            k_eff = int(max(1, min(int(spline_order), m - 1)))
            try:
                spline = UnivariateSpline(t_valid, y_valid, k=k_eff, s=0)
                return spline(t_new)
            except Exception:
                # Fallback to linear interpolation
                self.log_warning_message("UnivariateSpline failed, falling back to linear interpolation")
                return np.interp(t_new, t_valid, y_valid)

        # Should never reach here
        raise RuntimeError("Interpolation dispatch fell through.")

    def interpolate_one(
        self,
        df: pd.DataFrame,
        t_new: np.ndarray,
        time_col: str,
        method: str = "makima",
        spline_order: int = 3,
        edge_strategy: str = "nearest",
        min_values_threshold: int = None,
    ) -> pd.DataFrame:
        """Interpolate a single DataFrame"""
        t = df[time_col].to_numpy()
        numeric_cols = [c for c in df.columns if c != time_col and pd.api.types.is_numeric_dtype(df[c])]

        out = pd.DataFrame(index=pd.Index(t_new, name=time_col))
        interpolated_columns = []  # Track which columns were actually interpolated

        for col in numeric_cols:
            y = df[col].to_numpy()
            mask = np.isfinite(t) & np.isfinite(y)
            t_valid, y_valid = t[mask], y[mask]

            # Check if column has enough non-NaN values for interpolation
            if min_values_threshold is not None:
                if t_valid.size < min_values_threshold:
                    # Skip interpolation for this column - leave it empty (NaN)
                    out[col] = np.nan
                    continue

            if t_valid.size < 2:
                if t_valid.size == 1:
                    out[col] = np.full_like(t_new, y_valid[0], dtype=float)
                continue

            # deduplicate time by averaging, then sort
            t_valid, y_valid = self._dedup_time_average(t_valid, y_valid)
            order = np.argsort(t_valid)
            t_valid = t_valid[order]
            y_valid = y_valid[order]

            # core interpolation (no edges yet)
            y_core = self._core_interpolate(t_valid, y_valid, t_new, method=method, spline_order=spline_order)

            # edges policy (consistent across methods)
            left_mask = t_new < t_valid[0]
            right_mask = t_new > t_valid[-1]
            if left_mask.any() or right_mask.any():
                if edge_strategy == "nearest":
                    y_core[left_mask] = y_valid[0]
                    y_core[right_mask] = y_valid[-1]
                elif edge_strategy == "linear":
                    if t_valid.size >= 2:
                        dtL = (t_valid[1] - t_valid[0])
                        mL = (y_valid[1] - y_valid[0]) / (dtL if dtL != 0 else 1)
                        y_core[left_mask] = y_valid[0] + mL * (t_new[left_mask] - t_valid[0])

                        dtR = (t_valid[-1] - t_valid[-2])
                        mR = (y_valid[-1] - y_valid[-2]) / (dtR if dtR != 0 else 1)
                        y_core[right_mask] = y_valid[-1] + mR * (t_new[right_mask] - t_valid[-1])
                elif edge_strategy == "nan":
                    y_core[left_mask] = np.nan
                    y_core[right_mask] = np.nan

            out[col] = y_core
            interpolated_columns.append(col)  # Mark as interpolated

        # Store interpolated columns info in DataFrame metadata
        out.attrs['interpolated_columns'] = interpolated_columns
        return out

    def run(self, params: ConfigParams, inputs) -> Dict[str, Any]:
        resource_set: ResourceSet = inputs['resource_set']

        # Get parameters
        time_col = params.get_value('time_column')
        batch_col = params.get_value('batch_column')
        sample_col = params.get_value('sample_column')
        method = params.get_value('method')
        grid_strategy = params.get_value('grid_strategy')
        n_points = params.get_value('n_points')
        spline_order = params.get_value('spline_order')
        edge_strategy = params.get_value('edge_strategy')
        reference_index = params.get_value('reference_index')
        min_values_threshold = params.get_value('min_values_threshold')

        self.log_info_message(f"Starting interpolation with method: {method}, grid strategy: {grid_strategy}")
        if min_values_threshold is not None:
            self.log_info_message(f"Only interpolating columns with at least {min_values_threshold} non-NaN values")

        # Prepare data
        resources = resource_set.get_resources()
        raw_dfs = {}
        metadata = {}

        for resource_name, resource in resources.items():
            if not isinstance(resource, Table):
                continue

            df = self.prepare_dataframe(resource.get_data(), time_col)
            raw_dfs[resource_name] = df

            # Store metadata
            metadata[resource_name] = {
                'original_resource': resource,
                'tags': resource.tags,
                'column_tags': {col: resource.get_column_tags_by_name(col) for col in resource.get_column_names()}
            }

        if not raw_dfs:
            raise ValueError("No valid Table resources found for interpolation")

        dfs = list(raw_dfs.values())

        # Build time grid based on strategy
        if grid_strategy == "per_file":
            # Each file gets its own grid
            results = {}
            for name, df in raw_dfs.items():
                t_array = df[time_col].to_numpy()
                local_n = self.auto_n_points_from_time_list([t_array])
                t_new_local = np.linspace(float(np.nanmin(t_array)), float(np.nanmax(t_array)), int(local_n))
                results[name] = self.interpolate_one(df, t_new_local, time_col, method=method,
                                                     spline_order=spline_order, edge_strategy=edge_strategy,
                                                     min_values_threshold=min_values_threshold)
                results[name].index.name = time_col
        elif grid_strategy == "reference":
            # Use reference file's time range
            if reference_index >= len(dfs):
                reference_index = 0
                self.log_warning_message("Reference index out of range, using index 0")
            ref_df = dfs[reference_index]
            t_new = self.build_global_grid([ref_df], time_col, n_points)
            results = {}
            for name, df in raw_dfs.items():
                results[name] = self.interpolate_one(
                    df, t_new, time_col, method=method, spline_order=spline_order, edge_strategy=edge_strategy,
                    min_values_threshold=min_values_threshold)
        else:  # global_auto
            # Use global time range
            t_new = self.build_global_grid(dfs, time_col, n_points)
            results = {}
            for name, df in raw_dfs.items():
                results[name] = self.interpolate_one(
                    df, t_new, time_col, method=method, spline_order=spline_order, edge_strategy=edge_strategy,
                    min_values_threshold=min_values_threshold)

        # Create output ResourceSet with combined data
        subsampled_res = ResourceSet()

        for resource_name, interpolated_df in results.items():
            # Get original data
            original_resource = resources[resource_name]
            original_df = original_resource.get_data()

            # Get interpolated columns list
            interpolated_columns = interpolated_df.attrs.get('interpolated_columns', [])

            # Reset index to make time column a regular column for interpolated data
            interpolated_df_with_time = interpolated_df.reset_index()

            # Create combined DataFrame
            # Start with all time points (both real and interpolated)
            combined_times = pd.concat([
                original_df[time_col],
                interpolated_df_with_time[time_col]
            ]).drop_duplicates().sort_values().reset_index(drop=True)

            combined_df = pd.DataFrame({time_col: combined_times})

            # Get all columns (data + index columns)
            all_columns = original_df.columns.tolist()
            all_columns.remove(time_col)  # Remove time column as we already have it

            # Remove batch and sample columns from the list if they exist
            columns_to_exclude = []
            if batch_col and batch_col in all_columns:
                columns_to_exclude.append(batch_col)
            if sample_col and sample_col in all_columns:
                columns_to_exclude.append(sample_col)

            for col_to_exclude in columns_to_exclude:
                all_columns.remove(col_to_exclude)

            # For each column, decide whether to use real or interpolated data
            for col in all_columns:
                if col in interpolated_columns:
                    # Column was interpolated - use interpolated values
                    # Merge interpolated data on time
                    temp_df = combined_df.merge(
                        interpolated_df_with_time[[time_col, col]],
                        on=time_col,
                        how='left'
                    )
                    combined_df[col] = temp_df[col]
                else:
                    # Column was NOT interpolated - use real values only
                    # Merge original data on time
                    temp_df = combined_df.merge(
                        original_df[[time_col, col]],
                        on=time_col,
                        how='left'
                    )
                    combined_df[col] = temp_df[col]

            # Remove rows where all columns except time column are NaN
            non_time_cols = [col for col in combined_df.columns if col != time_col]
            if non_time_cols:
                combined_df = combined_df.dropna(subset=non_time_cols, how='all')

            # Create new Table with combined data
            combined_table = Table(combined_df)
            combined_table.name = f"{resource_name}_subsampled"

            # Copy tags from original resource
            original_meta = metadata[resource_name]
            if original_meta['tags']:
                for tag in original_meta['tags'].get_tags():
                    combined_table.tags.add_tag(Tag(key=tag.key, value=tag.value))

            # Add subsampling info tags
            combined_table.tags.add_tag(Tag(key="subsampling_method", value=method))
            combined_table.tags.add_tag(Tag(key="subsampling_grid_strategy", value=grid_strategy))
            combined_table.tags.add_tag(Tag(key="subsampling_edge_strategy", value=edge_strategy))

            # Copy column tags for all columns
            for col in combined_df.columns:
                if col in original_meta['column_tags']:
                    col_tags = original_meta['column_tags'][col]
                    for tag_key, tag_value in col_tags.items():
                        combined_table.add_column_tag_by_name(col, tag_key, tag_value)

            # Add tag to columns that were interpolated
            for col in interpolated_columns:
                if col in combined_df.columns:
                    combined_table.add_column_tag_by_name(col, "is_interpolated", "true")

            subsampled_res.add_resource(combined_table, resource_name)

        self.log_success_message(f"Successfully subsampled {len(results)} resources using {method} method")

        return {
            'subsampled_resource_set': subsampled_res
        }
